{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "#from pandas.tests.frame.methods.test_sort_values import ascending\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from pandas.tests.frame.methods.test_sort_values import ascending\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/mnt/d/learn/DE/Semina_project/spark\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField,StringType,ArrayType\n",
    "from pyspark.sql.functions import input_file_name, explode, col,lit, date_format, to_timestamp\n",
    "import duckdb\n",
    "import pyarrow as pa"
   ],
   "id": "7948c80c40a3d59c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "def get_latest_file(bucket_name, prefix,days_lookback):\n",
    "    s3 = boto3.client(\n",
    "        \"s3\",\n",
    "        endpoint_url=S3_ENDPOINT,\n",
    "        aws_access_key_id=S3_ACCESS,\n",
    "        aws_secret_access_key=S3_SECRET,\n",
    "        region_name=\"us-east-1\"\n",
    "    )\n",
    "    all_objects = []\n",
    "    # Paginator is used to list if more 1000 files\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=prefix)\n",
    "    for page in page_iterator:\n",
    "        for obj in page.get('Contents', []):\n",
    "            key=obj['Key']\n",
    "            last_modified = obj['LastModified']\n",
    "            if last_modified > datetime.now(timezone.utc)- timedelta(days=days_lookback):\n",
    "                 all_objects.append(obj)\n",
    "    all_objects.sort(key=lambda x: x['LastModified'], reverse=True)\n",
    "    #latest_files = all_objects[:limit]\n",
    "    #latest_files = all_objects\n",
    "    paths = [f\"s3a://{bucket_name}/{obj['Key']}\" for obj in all_objects]\n",
    "    return paths\n",
    "\n",
    "latest_files = get_latest_file(\n",
    "        bucket_name=S3_BUCKET,\n",
    "        prefix=\"bronze/okx_trades\",\n",
    "        days_lookback=1 # thay ƒë·ªïi khi run l·∫°i to√†n b·ªô\n",
    ")\n",
    "latest_files"
   ],
   "id": "f6794b21713da3c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OKX_Bronze_To_Silver_Book_new\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "da62bae69543c77b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trade_schema = StructType([\n",
    "    StructField(\"instId\", StringType(), True),\n",
    "    StructField(\"tradeId\", StringType(), True),\n",
    "    StructField(\"px\", StringType(), True),\n",
    "    StructField(\"sz\", StringType(), True),\n",
    "    StructField(\"side\", StringType(), True),\n",
    "    StructField(\"ts\", StringType(), True),\n",
    "    StructField(\"count\", StringType(), True),\n",
    "    StructField(\"source\", StringType(), True),\n",
    "    StructField(\"seqId\", StringType(), True)\n",
    "])\n",
    "schema = StructType([\n",
    "    StructField(\"received_at\", StringType(), True),\n",
    "    StructField(\"payload\", StructType([\n",
    "        StructField(\"data\", ArrayType(trade_schema), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "df = spark.read.schema(schema).json(latest_files)\n",
    "df.show(truncate=False)"
   ],
   "id": "da20810c52270ec4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_exploded = df.select(\n",
    "    col(\"received_at\"),\n",
    "    col(\"payload.arg.instId\").alias(\"symbol\"),\n",
    "    col(\"payload.arg.channel\").alias(\"channel\"),  # V√≠ d·ª•: candle1s, candle1m\n",
    "    explode(col(\"payload.data\")).alias(\"candle\")  # M·ªói d√≤ng l√† 1 m·∫£ng [\"ts\", \"o\", ...]\n",
    ")\n",
    "df_silver = df_exploded.select(\n",
    "    col(\"symbol\"),\n",
    "    col(\"channel\"),\n",
    "    to_timestamp(col(\"candle\")[0].cast(\"long\") / 1000).alias(\"candle_time\"),\n",
    "    col(\"candle\")[1].cast(\"double\").alias(\"open\"),\n",
    "    col(\"candle\")[2].cast(\"double\").alias(\"high\"),\n",
    "    col(\"candle\")[3].cast(\"double\").alias(\"low\"),\n",
    "    col(\"candle\")[4].cast(\"double\").alias(\"close\"),\n",
    "    col(\"candle\")[5].cast(\"double\").alias(\"volume\"),\n",
    "    col(\"candle\")[8].cast(\"int\").alias(\"is_confirmed\"),\n",
    "    col(\"received_at\").cast(\"timestamp\").alias(\"ingestion_time\")\n",
    ")\n",
    "df_silver.show()"
   ],
   "id": "71e3d6a447b36d4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#df.show(truncate=False)\n",
    "\n",
    "#df_exploded.show(truncate=False)\n",
    "# B3.2: √âp ki·ªÉu v√† Ch·ªçn c·ªôt\n",
    "\n",
    "#df_silver.show(truncate=False)\n",
    "df_cleaned = df_silver \\\n",
    "    .dropna(subset=[\"open\", \"candle_time\"]) \\\n",
    "    .withColumn(\"date_part\", date_format(col(\"candle_time\"), \"yyyy-MM-dd\"))\n",
    "df_cleaned.show()\n"
   ],
   "id": "740652257d91478a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_cleaned.printSchema()",
   "id": "7f100dd0980c07ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "output = f\"s3a://trading-okx/silver/okx-funding/\"\n",
    "df_silver.write.mode('append').format(\"parquet\").save(output)"
   ],
   "id": "363e3b1d56088f38",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# config dw duckdb",
   "id": "4a5505312eb9b69b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# connect to duckdb\n",
    "duck_path='/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "con=duckdb.connect(duck_path)"
   ],
   "id": "2476a25c19f0c7be",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "con.close()",
   "id": "a3eae653ba3e4b71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# scrip create table\n",
    "with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_database/source_db.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "con.execute(sql_script)"
   ],
   "id": "ab0804d8c4a4dc6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test table\n",
    "con.sql(\"show TABLEs ; \")\n",
    "#con.sql(\"select * from fact_ohlc;\")\n"
   ],
   "id": "5b3121194beca17c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read file sql to connect to minio and create dim fact table\n",
    "with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "con.execute(sql_script)"
   ],
   "id": "fd9acd1b65198b18",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "arrow_table = pa.Table.from_pandas(df_cleaned.toPandas())\n",
    "con.register(\"arrow_table\", arrow_table)\n",
    "con.sql('select * from arrow_table')\n",
    "#con.close()"
   ],
   "id": "3593a6823e8bcdd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "con.execute('''\n",
    "                INSERT INTO fact_ohlc(\n",
    "                                            symbol,\n",
    "                                            channel,\n",
    "                                            candle_time,\n",
    "                                            open,\n",
    "                                            high,\n",
    "                                            low,\n",
    "                                            close,\n",
    "                                            volume,\n",
    "                                            is_confirmed,\n",
    "                                            ingestion_time,\n",
    "                                            date_part)\n",
    "                SELECT\n",
    "                    symbol ,\n",
    "                    channel ,\n",
    "                    candle_time ,\n",
    "                    open ,\n",
    "                    high ,\n",
    "                    low ,\n",
    "                    close ,\n",
    "                    volume ,\n",
    "                    is_confirmed ,\n",
    "                    ingestion_time ,\n",
    "                    date_part\n",
    "                FROM arrow_table\n",
    "                ''')"
   ],
   "id": "192ad9190d9f3b84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read file sql to connect to minio and create dim fact table\n",
    "with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "    sql_script = f.read()\n",
    "con.execute(sql_script)\n",
    "\n",
    "output_path = f\"s3://trading-okx/silver/okx-funding/*.parquet\"\n",
    "try:\n",
    "    # Ch·ªâ load d·ªØ li·ªáu m·ªõi (V√≠ d·ª• l·ªçc theo ng√†y n·∫øu c·∫ßn)\n",
    "    # ·ªû ƒë√¢y load h·∫øt (Full Load)\n",
    "    con.execute(f\"\"\"\n",
    "        INSERT INTO fact_funding_rate\n",
    "        SELECT * FROM read_parquet('{output_path}')\n",
    "       --WHERE ingestion_time NOT IN (SELECT ingestion_time FROM fact_orders_books) -- Tr√°nh tr√πng l·∫∑p (Dedup ƒë∆°n gi·∫£n)\n",
    "    \"\"\")\n",
    "\n",
    "    # L·∫•y s·ªë d√≤ng ƒë√£ insert\n",
    "    count = con.execute(\"SELECT count(*) FROM fact_funding_rate\").fetchone()[0]\n",
    "    print(f\"‚úÖ Data loaded. Total rows in DuckDB: {count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n"
   ],
   "id": "2aca4480477753e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "con.close()\n",
    "spark.stop()"
   ],
   "id": "9ac9823548815d76",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "import os, duckdb\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, window, max, min, sum,\n",
    "    struct, date_format, lit)\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timezone, timedelta\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "ACCESS_KEY='minio'\n",
    "SECRET_KEY= 'minio123'\n",
    "S3_BUCKET='trading-okx'\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL_Trades_To_OHLC_Aggregator\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ],
   "id": "9a7f12a2ed49d4e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input=f's3a://{S3_BUCKET}/silver/trades/*/*.parquet'\n",
    "df = spark.read.parquet(input)\n",
    "df.show(5, False)"
   ],
   "id": "4def98a24d922c99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.printSchema()",
   "id": "9f7e07fd472654d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "CHECKPOINT_ROOT = f\"s3a://{S3_BUCKET}/checkpoints/ohlc_parquet_v1/\"\n",
    "WATERMARK_DELAY = \"10 minutes\"\n",
    "interval={\n",
    "    \"1m\": \"1 minute\",\n",
    "    \"5m\": \"5 minutes\",\n",
    "    \"15m\": \"15 minutes\",\n",
    "    # \"1h\": \"1 hour\",\n",
    "    # \"4h\": \"4 hours\",\n",
    "    # \"1d\": \"1 day\"\n",
    "}\n",
    "today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "process_date_str=today\n",
    "input_path = f\"s3a://{S3_BUCKET}/silver/trades/date_part={process_date_str}/*.parquet\"\n",
    "\n",
    "df_trades = spark.read.parquet(input_path)\n",
    "\n",
    "for interval_name, interval_window in interval.items():\n",
    "    print(f\"   ‚è≥ T√≠nh to√°n khung: {interval_name}...\")\n",
    "    df_ohlc = df_trades.groupBy(\n",
    "        col(\"symbol\"),\n",
    "        window(col(\"trade_time\"), interval_window).alias(\"window_time\")\n",
    "    ).agg(\n",
    "        min(struct(col(\"trade_time\"), col(\"price\"))).getItem(\"price\").alias(\"open\"),\n",
    "        max(\"price\").alias(\"high\"),\n",
    "        min(\"price\").alias(\"low\"),\n",
    "        max(struct(col(\"trade_time\"), col(\"price\"))).getItem(\"price\").alias(\"close\"),\n",
    "        sum(\"quantity\").alias(\"volume\")\n",
    "    )\n",
    "    df_final = df_ohlc.select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"window_time.start\").alias(\"candle_time\"),\n",
    "        col(\"open\"), col(\"high\"), col(\"low\"), col(\"close\"), col(\"volume\"),\n",
    "        lit(interval_name).alias(\"interval\"),\n",
    "        # T·∫°o l·∫°i c·ªôt date_part ƒë·ªÉ Spark bi·∫øt ghi v√†o ƒë√¢u\n",
    "        date_format(col(\"window_time.start\"), \"yyyy-MM-dd\").alias(\"date_part\")\n",
    "    )\n",
    "    output_path = f\"s3a://{S3_BUCKET}/silver/calculated_ohlc/\"\n",
    "\n",
    "    # QUAN TR·ªåNG: D√πng mode(\"overwrite\") + partitionOverwriteMode=dynamic\n",
    "    # N√≥ s·∫Ω ch·ªâ thay th·∫ø th∆∞ m·ª•c c·ªßa ng√†y {process_date_str} b√™n trong output_path\n",
    "    # D·ªØ li·ªáu ng√†y c≈© v·∫´n an to√†n.\n",
    "\n",
    "    df_final.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"interval\", \"date_part\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .save(output_path)\n",
    "\n",
    "    print(f\"      ‚úÖ ƒê√£ l∆∞u (Overwrite) partition: {process_date_str}\")"
   ],
   "id": "e1068b036d66feb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_final.show(10,False)",
   "id": "a45d0e13dca63c95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ohlc from trades",
   "id": "d4d0c9666385aad6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, max, min, sum, struct, lit, date_format, current_timestamp\n",
    "\n",
    "# --- CONFIG ---\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\""
   ],
   "id": "283098f7c3c91097",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, max, min, sum, struct, lit, date_format, current_timestamp\n",
    "\n",
    "# --- CONFIG ---\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "\n",
    "# ƒê∆∞·ªùng d·∫´n Checkpoint (B·∫ÆT BU·ªòC PH·∫¢I C√ì ƒë·ªÉ resume)\n",
    "# Spark s·∫Ω l∆∞u tr·∫°ng th√°i v√†o ƒë√¢y. Tuy·ªát ƒë·ªëi kh√¥ng x√≥a th∆∞ m·ª•c n√†y n·∫øu mu·ªën ch·∫°y ti·∫øp.\n",
    "CHECKPOINT_ROOT = f\"s3a://{S3_BUCKET}/checkpoints/ohlc_parquet_v1/\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "# C·∫•u h√¨nh ƒë·ªô tr·ªÖ cho ph√©p (Watermark)\n",
    "# D·ªØ li·ªáu ƒë·∫øn mu·ªôn qu√° 10 ph√∫t s·∫Ω b·ªã b·ªè qua, n·∫øn s·∫Ω ƒë√≥ng sau 10 ph√∫t.\n",
    "WATERMARK_DELAY = \"10 minutes\"\n",
    "def get_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"OKX_Bronze_To_Silver_trade\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"1\") \\\n",
    "        .getOrCreate()\n",
    "def get_duckdb_conn():\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "             sql_script = f.read()\n",
    "    con.execute(sql_script)\n",
    "    return con\n",
    "def load_to_duckdb(interval_name):\n",
    "    \"\"\"\n",
    "    H√†m n√†y ch·∫°y SAU KHI Spark ƒë√£ ghi xong file Parquet.\n",
    "    N√≥ ra l·ªánh cho DuckDB ƒë·ªçc file Parquet m·ªõi v√† insert v√†o b·∫£ng.\n",
    "    \"\"\"\n",
    "    print(f\"ü¶Ü [DuckDB] ƒêang n·∫°p d·ªØ li·ªáu OHLC ({interval_name}) v√†o Warehouse...\")\n",
    "    con = get_duckdb_conn()\n",
    "\n",
    "    # Spark partition theo: interval=1h/date_part=2023-12-01\n",
    "    parquet_source = f\"s3://{S3_BUCKET}/silver/calculated_ohlc/interval={interval_name}/*/*.parquet\"\n",
    "\n",
    "    try:\n",
    "        # 2.1 Load v√†o Staging (Incremental)\n",
    "        # B·∫£ng staging: ohlc\n",
    "        # L∆∞u √Ω: Interval l√† hardcode t·ª´ tham s·ªë h√†m v√¨ Spark partition theo folder n√†y\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO ohlc (symbol, candle_time, open, high, low, close, volume, interval)\n",
    "            SELECT symbol, candle_time, open, high, low, close, volume, '{interval_name}'\n",
    "            FROM read_parquet('{parquet_source}', hive_partitioning=1)\n",
    "            WHERE candle_time > (SELECT COALESCE(MAX(candle_time), '1970-01-01'::TIMESTAMP) FROM ohlc WHERE interval = '{interval_name}')\n",
    "        \"\"\")\n",
    "        print(\"   ‚úÖ Staging Loaded.\")\n",
    "\n",
    "        # 2.2 Update Dim Time\n",
    "        # T·∫°o ng√†y m·ªõi n·∫øu ch∆∞a c√≥\n",
    "        con.execute(\"\"\"\n",
    "            INSERT INTO dim_time\n",
    "            SELECT DISTINCT\n",
    "                CAST(strftime(candle_time, '%Y%m%d') AS INTEGER) as date_key,\n",
    "                CAST(candle_time AS DATE),\n",
    "                EXTRACT(YEAR FROM candle_time), EXTRACT(QUARTER FROM candle_time),\n",
    "                EXTRACT(MONTH FROM candle_time), EXTRACT(DAY FROM candle_time),\n",
    "                ISODOW(candle_time), CASE WHEN ISODOW(candle_time) IN (6, 7) THEN TRUE ELSE FALSE END\n",
    "            FROM ohlc\n",
    "            WHERE CAST(strftime(candle_time, '%Y%m%d') AS INTEGER) NOT IN (SELECT date_key FROM dim_time)\n",
    "        \"\"\")\n",
    "\n",
    "        # 2.3 Insert Fact Table (fact_ohlc_calculated)\n",
    "        # JOIN v·ªõi dim_symbol v√† dim_time ƒë·ªÉ l·∫•y Key chu·∫©n\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO fact_ohlc_calculated (symbol_key, date_key, interval, candle_time, open, high, low, close, volume)\n",
    "            SELECT\n",
    "                d.symbol_key,\n",
    "                CAST(strftime(s.candle_time, '%Y%m%d') AS INTEGER) as date_key,\n",
    "                s.interval,\n",
    "                s.candle_time,\n",
    "                s.open, s.high, s.low, s.close, s.volume\n",
    "            FROM ohlc s\n",
    "            JOIN dim_symbol d ON s.symbol = d.symbol_code\n",
    "            WHERE s.candle_time > (SELECT COALESCE(MAX(candle_time), '1970-01-01'::TIMESTAMP) FROM fact_ohlc_calculated WHERE interval = '{interval_name}')\n",
    "            AND s.interval = '{interval_name}'\n",
    "        \"\"\")\n",
    "        print(\"   ‚úÖ Gold (Fact) Loaded.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è DuckDB Load Error: {e}\")\n",
    "        # Kh√¥ng raise ƒë·ªÉ pipeline ch·∫°y ti·∫øp interval kh√°c\n",
    "    finally:\n",
    "        con.close()\n"
   ],
   "id": "cd7fb80f4e4f0971",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "def run_streaming_ohlc(spark, interval_name=\"1m\", interval_window=\"1 minute\"):\n",
    "    print(f\"üöÄ ƒêang x·ª≠ l√Ω khung th·ªùi gian: {interval_name}\")\n",
    "\n",
    "    # 1. INPUT: READ STREAM (Ch·ªâ ƒë·ªçc file m·ªõi)\n",
    "    # Spark t·ª± theo d√µi file n√†o m·ªõi trong th∆∞ m·ª•c n√†y\n",
    "    input_path = f\"s3a://{S3_BUCKET}/silver/trades/\"\n",
    "\n",
    "    # L·∫•y schema t·ª´ 1 file m·∫´u (ƒë·ªÉ tr√°nh l·ªói schema evolution)\n",
    "    try:\n",
    "        schema = spark.read.parquet(input_path).schema\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è Ch∆∞a c√≥ data trades. Tho√°t.\")\n",
    "        return\n",
    "\n",
    "    df_trades = spark.readStream \\\n",
    "        .schema(schema) \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"maxFilesPerTrigger\", 1000) \\\n",
    "        .load(input_path)\n",
    "\n",
    "    # 2. TRANSFORM: AGGREGATE V·ªöI WATERMARK\n",
    "    # B·∫Øt bu·ªôc ph·∫£i c√≥ withWatermark ƒë·ªÉ d√πng mode 'append'\n",
    "\n",
    "    df_ohlc = df_trades \\\n",
    "        .withWatermark(\"trade_time\", WATERMARK_DELAY) \\\n",
    "        .groupBy(\n",
    "            col(\"symbol\"),\n",
    "            window(col(\"trade_time\"), interval_window).alias(\"window_time\")\n",
    "        ).agg(\n",
    "            min(struct(col(\"trade_time\"), col(\"price\"))).getItem(\"price\").alias(\"open\"),\n",
    "            max(\"price\").alias(\"high\"),\n",
    "            min(\"price\").alias(\"low\"),\n",
    "            max(struct(col(\"trade_time\"), col(\"price\"))).getItem(\"price\").alias(\"close\"),\n",
    "            sum(\"quantity\").alias(\"volume\")\n",
    "        )\n",
    "\n",
    "    # Chu·∫©n h√≥a ƒë·∫ßu ra\n",
    "    df_final = df_ohlc.select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"window_time.start\").alias(\"candle_time\"),\n",
    "        col(\"open\"), col(\"high\"), col(\"low\"), col(\"close\"), col(\"volume\"),\n",
    "        lit(interval_name).alias(\"interval\"),\n",
    "        date_format(col(\"window_time.start\"), \"yyyy-MM-dd\").alias(\"date_part\")\n",
    "    )\n",
    "\n",
    "    # 3. OUTPUT: WRITE STREAM (APPEND ONLY)\n",
    "    # Output path ri√™ng cho t·ª´ng interval\n",
    "    output_path = f\"s3a://{S3_BUCKET}/silver/calculated_ohlc/\"\n",
    "    checkpoint_path = f\"{CHECKPOINT_ROOT}/{interval_name}\"\n",
    "    # Checkpoint ri√™ng cho t·ª´ng interval (Quan tr·ªçng!)\n",
    "\n",
    "    query = df_final.writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .trigger(availableNow=True) \\\n",
    "        .partitionBy(\"interval\", \"date_part\") \\\n",
    "        .start(output_path)\n",
    "\n",
    "    # trigger(availableNow=True):\n",
    "    # - ƒê·ªçc h·∫øt data m·ªõi -> T√≠nh to√°n -> Ghi xu·ªëng Parquet -> L∆∞u Checkpoint -> Stop.\n",
    "    # - Kh√¥ng treo m√°y ch·ªù data nh∆∞ streaming th√¥ng th∆∞·ªùng.\n",
    "\n",
    "    query.awaitTermination()\n",
    "\n",
    "    print(f\"‚úÖ Ho√†n t·∫•t x·ª≠ l√Ω {interval_name}. Data ƒë√£ ƒë∆∞·ª£c Append v√†o MinIO.\")\n",
    "    load_to_duckdb(interval_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # B·∫°n c√≥ th·ªÉ ch·∫°y nhi·ªÅu interval\n",
    "    intervals = [\n",
    "        (\"1m\", \"1 minute\"),\n",
    "        (\"5m\", \"5 minute\")\n",
    "    ]\n",
    "\n",
    "    for name, window_duration in intervals:\n",
    "        run_streaming_ohlc(spark, name, window_duration)\n",
    "\n",
    "    spark.stop()"
   ],
   "id": "3663d7a6a4c76dcc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "con = duckdb.connect('/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb')\n",
    "'''with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "     sql_script = f.read()\n",
    "con.execute(sql_script)'''\n",
    "##print(con.execute('drop table fact_ohlc; '))\n",
    "#print(con.sql('PRAGMA show_tables;'))\n",
    "print(con.sql('select * from trades'))\n",
    "print(con.sql('select * from fact_trades'))\n",
    "\n",
    "con.close()"
   ],
   "id": "f58f4a482af39b0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_complex_features(spark, date_process, interval_name=\"5m\", interval_window=\"5 minutes\"):\n",
    "    print(f\"üöÄ Processing Features for Date: {date_process} | Interval: {interval_name}\")\n",
    "\n",
    "    input_path = f\"s3a://{S3_BUCKET}/silver/trades/date_part={date_process}/*.parquet\"\n",
    "    try:\n",
    "        df = spark.read.parquet(input_path)\n",
    "    except:\n",
    "        print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y data ng√†y {date_process}\")\n",
    "        return False\n",
    "    # 2. Pre-calculation (T√≠nh c√°c c·ªôt ph·ª• tr·ª£ tr∆∞·ªõc khi gom nh√≥m)\n",
    "\n",
    "    df = df.withColumn(\"turnover\", F.col(\"price\") * F.col(\"quantity\"))\n",
    "    # groupby symbol\n",
    "    features_df = df.groupBy(\n",
    "        F.col(\"symbol\"),\n",
    "        F.window(F.col(\"trade_time\"), interval_window).alias(\"window\") # window t·∫°o c·ª≠a s·ªï: [00:00,01:00), [01:00,02:00)\n",
    "    ).agg(\n",
    "        # --- A. BASIC OHLCV ---\n",
    "        F.min(F.struct(\"trade_time\", \"price\")).getItem(\"price\").alias(\"open\"),\n",
    "        F.max(\"price\").alias(\"high\"),\n",
    "        F.min(\"price\").alias(\"low\"),\n",
    "        F.max(F.struct(\"trade_time\", \"price\")).getItem(\"price\").alias(\"close\"),\n",
    "        F.sum(\"quantity\").alias(\"volume\"),\n",
    "        F.sum(\"turnover\").alias(\"total_turnover\"),\n",
    "        F.count(\"*\").alias(\"trade_count\"),\n",
    "\n",
    "        # --- B. BUY/SELL PRESSURE (√Åp l·ª±c mua b√°n) ---\n",
    "        F.sum(F.when(F.col(\"side\") == \"buy\", F.col(\"quantity\")).otherwise(0)).alias(\"vol_buy\"),\n",
    "        F.sum(F.when(F.col(\"side\") == \"sell\", F.col(\"quantity\")).otherwise(0)).alias(\"vol_sell\"),\n",
    "        F.count(F.when(F.col(\"side\") == \"buy\", 1)).alias(\"count_buy\"),\n",
    "        F.count(F.when(F.col(\"side\") == \"sell\", 1)).alias(\"count_sell\"),\n",
    "\n",
    "        # --- C. STATISTICAL FEATURES (Ph√¢n ph·ªëi gi√°) ---\n",
    "        # Polars: std, skew, kurtosis\n",
    "        F.stddev(\"price\").alias(\"price_std\"),\n",
    "        F.skewness(\"price\").alias(\"price_skew\"),\n",
    "        F.kurtosis(\"price\").alias(\"price_kurtosis\"),\n",
    "\n",
    "        # Polars: quantiles (0.25, 0.5, 0.75)\n",
    "        # percentile_approx l√† h√†m x·∫•p x·ªâ r·∫•t nhanh tr√™n Big Data\n",
    "        F.percentile_approx(\"price\", 0.25).alias(\"price_q25\"),\n",
    "        F.percentile_approx(\"price\", 0.50).alias(\"price_median\"),\n",
    "        F.percentile_approx(\"price\", 0.75).alias(\"price_q75\"),\n",
    "\n",
    "        # --- D. SIZE DISTRIBUTION (Ph√¢n ph·ªëi kh·ªëi l∆∞·ª£ng l·ªánh) ---\n",
    "        F.max(\"quantity\").alias(\"size_max\"),\n",
    "        F.avg(\"quantity\").alias(\"size_mean\"),\n",
    "        F.stddev(\"quantity\").alias(\"size_std\")\n",
    "    )\n",
    "\n",
    "    # 4. POST-CALCULATION (T√≠nh to√°n tr√™n k·∫øt qu·∫£ ƒë√£ gom)\n",
    "    final_df = features_df.select(\n",
    "        F.col(\"symbol\"),\n",
    "        F.col(\"window.start\").alias(\"candle_time\"),\n",
    "        F.lit(interval_name).alias(\"interval\"),\n",
    "        # Basic\n",
    "        \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\",\n",
    "        # Advanced\n",
    "        \"vol_buy\", \"vol_sell\",\n",
    "        \"count_buy\", \"count_sell\",\n",
    "        (F.col(\"vol_buy\") - F.col(\"vol_sell\")).alias(\"net_volume\"), # Volume r√≤ng\n",
    "        # VWAP (Volume Weighted Average Price)\n",
    "        (F.col(\"total_turnover\") / F.col(\"volume\")).alias(\"vwap\"),\n",
    "        # Stats\n",
    "        \"price_std\", \"price_skew\", \"price_kurtosis\",\n",
    "        \"price_q25\", \"price_median\", \"price_q75\",\n",
    "        \"size_max\", \"size_mean\", \"size_std\",\n",
    "        # Partition Col\n",
    "        F.lit(date_process).alias(\"date_part\")\n",
    "    )\n",
    "\n",
    "    # 5. WRITE TO MINIO (Feature Store - Silver/Gold)\n",
    "    # L∆∞u √Ω: Overwrite partition c·ªßa ng√†y h√¥m ƒë√≥ ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n (Idempotency)\n",
    "    output_path = f\"s3a://{S3_BUCKET}/gold/features_ml/\"\n",
    "\n",
    "    print(f\"üíæ Saving ML Features to: {output_path}\")\n",
    "    final_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"interval\", \"date_part\") \\\n",
    "        .format(\"parquet\") \\\n",
    "        .save(output_path)\n",
    "\n",
    "    return True\n",
    "\n",
    "def load_features_to_duckdb(interval_name, date_process):\n",
    "    \"\"\"Load Features v√†o DuckDB Fact Table\"\"\"\n",
    "    print(f\"ü¶Ü Loading Features ({interval_name}) to DuckDB...\")\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "\n",
    "    # Setup MinIO\n",
    "    with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "             sql_script = f.read()\n",
    "    con.execute(sql_script)\n",
    "\n",
    "    # Path (Hive Partitioning)\n",
    "    parquet_path = f\"s3://{S3_BUCKET}/gold/features_ml/interval={interval_name}/date_part={date_process}/*.parquet\"\n",
    "\n",
    "    try:\n",
    "        # Create Table (N·∫øu ch∆∞a c√≥) - Schema si√™u to kh·ªïng l·ªì\n",
    "        # D√πng m·∫πo CREATE AS SELECT LIMIT 0 ƒë·ªÉ ƒë·ª° ph·∫£i g√µ tay schema\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS fact_market_features AS\n",
    "            SELECT * FROM read_parquet('{parquet_path}') LIMIT 0\n",
    "        \"\"\")\n",
    "\n",
    "        # X√≥a data c≈© c·ªßa ng√†y n√†y (ƒë·ªÉ tr√°nh double) -> C∆° ch·∫ø Overwrite\n",
    "        con.execute(f\"DELETE FROM fact_market_features WHERE interval='{interval_name}' AND date_part='{date_process}'\")\n",
    "\n",
    "        # Insert\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO fact_market_features\n",
    "            SELECT * FROM read_parquet('{parquet_path}')\n",
    "        \"\"\")\n",
    "        print(\"‚úÖ Features Loaded Successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è DuckDB Error: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # Config ng√†y ch·∫°y (Th∆∞·ªùng l√† ng√†y h√¥m qua T-1 ho·∫∑c ch·∫°y ƒë·ªãnh k·ª≥)\n",
    "    # V√≠ d·ª•: Run cho ng√†y 2025-12-06\n",
    "    target_date = \"2025-12-06\"\n",
    "\n",
    "    # Ch·∫°y cho nhi·ªÅu khung th·ªùi gian\n",
    "    intervals = [(\"5m\", \"5 minutes\"), (\"1h\", \"1 hour\")]\n",
    "\n",
    "    for name, window in intervals:\n",
    "        success = calculate_complex_features(spark, target_date, name, window)\n",
    "        if success:\n",
    "            load_features_to_duckdb(name, target_date)\n",
    "\n",
    "    spark.stop()"
   ],
   "id": "cf27d05c9eadbcd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "upgrade1: fit for training ML/AI",
   "id": "b1196acdb10a161"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# --- CONFIG ---\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "STAGING_OUTPUT_PATH = f\"s3a://{S3_BUCKET}/silver/agg_trades/\"\n",
    "\n",
    "def get_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Feature_Engineering_Job\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "def parse_interval_to_minutes(interval_str):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi chu·ªói interval ('1m', '1h', '1d') th√†nh s·ªë ph√∫t (int).\n",
    "    D√πng ƒë·ªÉ t√≠nh buffer an to√†n.\n",
    "    \"\"\"\n",
    "    unit = interval_str[-1].lower()\n",
    "    try:\n",
    "        value = int(interval_str[:-1])\n",
    "    except:\n",
    "        return 60 # Default fallback\n",
    "\n",
    "    if unit == 'm': return value\n",
    "    if unit == 'h': return value * 60\n",
    "    if unit == 'd': return value * 1440\n",
    "    return 60\n",
    "def get_last_processed_time(interval_name):\n",
    "    \"\"\"H·ªèi DuckDB xem l·∫ßn cu·ªëi t√≠nh feature l√† l√∫c n√†o\"\"\"\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    try:\n",
    "        # Ki·ªÉm tra b·∫£ng c√≥ t·ªìn t·∫°i kh√¥ng\n",
    "        table_exists = con.execute(\"SELECT count(*) FROM information_schema.tables WHERE table_name = 'fact_market_features'\").fetchone()[0]\n",
    "        if table_exists == 0:\n",
    "            return None\n",
    "\n",
    "        query = f\"SELECT MAX(candle_time) FROM fact_market_features WHERE interval = '{interval_name}'\"\n",
    "        result = con.execute(query).fetchone()[0]\n",
    "        return result # Tr·∫£ v·ªÅ datetime ho·∫∑c None\n",
    "    except:\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "def calculate_features_incremental(spark, interval_name, interval_window):\n",
    "    print(f\"\\nüöÄ Processing Features: {interval_name} ({interval_window})\")\n",
    "    # 1. T√çNH TO√ÅN BUFFER DYNAMIC (Quan tr·ªçng)\n",
    "    interval_minutes = parse_interval_to_minutes(interval_name)\n",
    "\n",
    "    # Quy t·∫Øc an to√†n: L√πi l·∫°i √≠t nh·∫•t 2 l·∫ßn ƒë·ªô d√†i n·∫øn + 10 ph√∫t tr·ªÖ\n",
    "    # V√≠ d·ª•: 12h -> L√πi 24h. 1m -> L√πi 12 ph√∫t.\n",
    "    buffer_minutes = (interval_minutes * 2) + 10\n",
    "\n",
    "    last_time = get_last_processed_time(interval_name)\n",
    "    cutoff_time = datetime.now() - timedelta(minutes=1)\n",
    "    input_path = f\"s3a://{S3_BUCKET}/silver/trades/\"\n",
    "\n",
    "    # ƒê·ªçc d·ªØ li·ªáu (T·ªëi ∆∞u: Ch·ªâ ƒë·ªçc c√°c file parquet c·∫ßn thi·∫øt n·∫øu c√≥ partition date)\n",
    "    # ·ªû ƒë√¢y ƒë·ªçc full folder r·ªìi filter (Spark s·∫Ω t·ª± t·ªëi ∆∞u ƒë·∫©y filter xu·ªëng)\n",
    "    df = spark.read.parquet(input_path)\n",
    "\n",
    "    # if last_time:\n",
    "    #     start_filter = last_time - timedelta(minutes=buffer_minutes)\n",
    "    #     print(f\"   ‚ÑπÔ∏è Incremental Mode: Reading trades after {start_filter} (Buffer: {buffer_minutes}m)\")\n",
    "    #     df = df.filter(F.col(\"trade_time\") >= F.lit(start_filter))\n",
    "    # else:\n",
    "    #     print(\"   ‚ÑπÔ∏è Full Load Mode: Reading all trades\")\n",
    "    if last_time:\n",
    "        # Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·∫ßn l·∫•y d·ªØ li·ªáu (tr·ª´ hao buffer)\n",
    "        start_timestamp = last_time - timedelta(minutes=buffer_minutes)\n",
    "\n",
    "        # --- K·ª∏ THU·∫¨T PARTITION PRUNING (QUAN TR·ªåNG) ---\n",
    "        # Chuy·ªÉn ƒë·ªïi timestamp th√†nh chu·ªói ng√†y (YYYY-MM-DD)\n",
    "        start_date_str = start_timestamp.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        print(f\"   ‚ÑπÔ∏è Incremental Mode: Reading from {start_timestamp}\")\n",
    "        print(f\"   üìÇ Partition Pruning: Reading folders from date_part >= {start_date_str}\")\n",
    "\n",
    "        # B∆∞·ªõc 1: L·ªçc Folder (Nhanh) - Spark s·∫Ω b·ªè qua c√°c folder c≈©\n",
    "        # date_part v·∫´n c√≥ trong file parquet d∆∞·ª£c ƒë∆∞a l√™n silver nh∆∞ng s·∫Ω ko insert v√†o table DB\n",
    "        df = df.filter(F.col(\"date_part\") >= F.lit(start_date_str))\n",
    "\n",
    "        # B∆∞·ªõc 2: L·ªçc chi ti·∫øt Time (Ch√≠nh x√°c) - L·∫•y ƒë√∫ng ph√∫t/gi√¢y\n",
    "        df = df.filter(F.col(\"trade_time\") >= F.lit(start_timestamp))\n",
    "\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è Full Load Mode: Reading all partitions\")\n",
    "    try:\n",
    "        # L·∫•y d√≤ng c√≥ th·ªùi gian nh·ªè nh·∫•t trong batch hi·ªán t·∫°i\n",
    "        min_row = df.select(F.min(\"trade_time\")).collect()\n",
    "        min_data_time = min_row[0][0]\n",
    "\n",
    "        if min_data_time is None:\n",
    "            print(\"   ‚ö†Ô∏è No data found in this range.\")\n",
    "            return False\n",
    "\n",
    "        print(f\"   üïí Batch Min Data Time: {min_data_time}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error checking min time: {e}\")\n",
    "        return False\n",
    "    # 2. T√≠nh to√°n Features (Gi·ªØ nguy√™n logic ph·ª©c t·∫°p c·ªßa b·∫°n)\n",
    "    df = df.withColumn(\"turnover\", F.col(\"price\") * F.col(\"quantity\"))\n",
    "\n",
    "    features_df = df.groupBy(\n",
    "        F.col(\"symbol\"),\n",
    "        F.window(F.col(\"trade_time\"), interval_window).alias(\"window\")\n",
    "    ).agg(\n",
    "        # Basic\n",
    "        F.min(F.struct(\"trade_time\", \"price\")).getItem(\"price\").alias(\"open\"),\n",
    "        F.max(\"price\").alias(\"high\"),\n",
    "        F.min(\"price\").alias(\"low\"),\n",
    "        F.max(F.struct(\"trade_time\", \"price\")).getItem(\"price\").alias(\"close\"),\n",
    "        F.sum(\"quantity\").alias(\"volume\"),\n",
    "        F.sum(\"turnover\").alias(\"total_turnover\"),\n",
    "        F.count(\"*\").alias(\"trade_count\"),\n",
    "        # Advanced (Skew, Kurtosis...)\n",
    "        F.stddev(\"price\").alias(\"price_std\"),\n",
    "        F.skewness(\"price\").alias(\"price_skew\"),\n",
    "        F.kurtosis(\"price\").alias(\"price_kurtosis\"),\n",
    "        # percentile_approx l√† h√†m x·∫•p x·ªâ r·∫•t nhanh tr√™n Big Data\n",
    "        F.percentile_approx(\"price\", 0.25).alias(\"price_q25\"),\n",
    "        F.percentile_approx(\"price\", 0.50).alias(\"price_median\"),\n",
    "        F.percentile_approx(\"price\", 0.75).alias(\"price_q75\"),\n",
    "        # Buy/Sell Volume\n",
    "        F.sum(F.when(F.col(\"side\") == \"buy\", F.col(\"quantity\")).otherwise(0)).alias(\"vol_buy\"),\n",
    "        F.sum(F.when(F.col(\"side\") == \"sell\", F.col(\"quantity\")).otherwise(0)).alias(\"vol_sell\"),\n",
    "        F.count(F.when(F.col(\"side\") == \"buy\", 1)).alias(\"count_buy\"),\n",
    "        F.count(F.when(F.col(\"side\") == \"sell\", 1)).alias(\"count_sell\"),\n",
    "        # --- D. SIZE DISTRIBUTION (Ph√¢n ph·ªëi kh·ªëi l∆∞·ª£ng l·ªánh) ---\n",
    "        F.max(\"quantity\").alias(\"size_max\"),\n",
    "        F.avg(\"quantity\").alias(\"size_mean\"),\n",
    "        F.stddev(\"quantity\").alias(\"size_std\")\n",
    "    )\n",
    "    features_df = features_df.filter(\n",
    "        (F.col(\"window.end\") <= F.lit(cutoff_time)) &\n",
    "        (F.col(\"window.start\") >= F.lit(min_data_time))\n",
    "    )\n",
    "\n",
    "    # Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu kh√¥ng sau khi l·ªçc\n",
    "    if features_df.rdd.isEmpty():\n",
    "        print(f\"   ‚ö†Ô∏è No closed candles found for {interval_name}. Waiting for more data...\")\n",
    "        return False\n",
    "    final_df = features_df.select(\n",
    "        F.col(\"symbol\"),\n",
    "        F.col(\"window.start\").alias(\"candle_time\"),\n",
    "        F.lit(interval_name).alias(\"interval\"),\n",
    "        \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\",\n",
    "        \"price_std\",\"price_skew\", \"price_kurtosis\", \"price_q25\",\"price_median\",\"price_q75\",\n",
    "        \"vol_buy\", \"vol_sell\",\"count_buy\",\"count_sell\",\"size_max\",\"size_mean\",\"size_std\",\n",
    "        (F.col(\"total_turnover\") / F.col(\"volume\")).alias(\"vwap\"),\n",
    "        F.current_timestamp().alias(\"ingestion_time\")\n",
    "    )\n",
    "\n",
    "    # 3. Ghi ra Staging Path (Ch·∫ø ƒë·ªô OVERWRITE cho folder staging n√†y th√¥i)\n",
    "    # Folder n√†y ch·ªâ ch·ª©a data c·ªßa l·∫ßn ch·∫°y n√†y, kh√¥ng ch·ª©a data c≈©\n",
    "    # Partition by date_part ƒë·ªÉ t·ªëi ∆∞u file size\n",
    "    #final_df = final_df.withColumn(\"date_part\", F.date_format(\"candle_time\", \"yyyy-MM-dd\"))\n",
    "\n",
    "    # Path ri√™ng cho interval n√†y trong staging\n",
    "    staging_path = f\"{STAGING_OUTPUT_PATH}/{interval_name}\"\n",
    "\n",
    "    print(f\"   üíæ Writing to Staging: {staging_path}\")\n",
    "    final_df.write.mode(\"overwrite\").parquet(staging_path)\n",
    "\n",
    "    return True\n",
    "\n",
    "def merge_to_duckdb(interval_name):\n",
    "    print(f\"ü¶Ü Merging {interval_name} into DuckDB...\")\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "        sql_script = f.read()\n",
    "    con.execute(sql_script)\n",
    "    staging_source = f\"s3://{S3_BUCKET}/silver/agg_trades/{interval_name}/*.parquet\"\n",
    "\n",
    "    try:\n",
    "        # Create Table (N·∫øu ch∆∞a c√≥)\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS fact_market_features AS\n",
    "            SELECT * FROM read_parquet('{staging_source}') LIMIT 0\n",
    "        \"\"\")\n",
    "\n",
    "        # X√≥a d·ªØ li·ªáu c≈© tr√πng l·∫∑p (D·ª±a tr√™n symbol, time V√Ä interval)\n",
    "        # V√¨ ta ƒëang ch·∫°y loop, ch·ªâ x√≥a nh·ªØng d√≤ng thu·ªôc interval ƒëang x·ª≠ l√Ω\n",
    "        print(\"   üîÑ Cleaning overlapping data...\")\n",
    "        con.execute(f\"\"\"\n",
    "            DELETE FROM fact_market_features\n",
    "            WHERE interval = '{interval_name}'\n",
    "            AND (symbol, candle_time) IN (\n",
    "                SELECT symbol, candle_time FROM read_parquet('{staging_source}')\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert m·ªõi\n",
    "        print(\"   üì• Inserting new data...\")\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO fact_market_features\n",
    "            SELECT * FROM read_parquet('{staging_source}')\n",
    "        \"\"\")\n",
    "\n",
    "        print(\"   ‚úÖ Merge Complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è DuckDB Merge Error: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # Danh s√°ch Interval c·∫ßn ch·∫°y\n",
    "    intervals = [\n",
    "        (\"1m\", \"1 minute\"),\n",
    "        (\"5m\", \"5 minutes\"),\n",
    "        (\"15m\", \"15 minutes\"),\n",
    "        (\"1h\", \"1 hour\"),\n",
    "        (\"4h\", \"4 hours\"),\n",
    "        (\"12h\", \"12 hours\") # Test interval l·ªõn\n",
    "    ]\n",
    "\n",
    "    print(f\"STARTING BATCH PIPELINE FOR {len(intervals)} INTERVALS...\")\n",
    "\n",
    "    for name, window in intervals:\n",
    "        # B∆∞·ªõc 1: T√≠nh to√°n\n",
    "        calculate_features_incremental(spark, name, window)\n",
    "        # B∆∞·ªõc 2: N·∫°p v√†o Fact\n",
    "        merge_to_duckdb(name)\n",
    "\n",
    "    spark.stop()"
   ],
   "id": "13b0b8dc9ec82383",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "con = duckdb.connect('/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb')\n",
    "with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "     sql_script = f.read()\n",
    "con.execute(sql_script)\n",
    "#rint(con.sql('PRAGMA show_tables;'))\n",
    "#print(con.execute('drop table fact_trades'))\n",
    "print(con.sql('select * from trades '))\n",
    "#print(con.sql('select * from fact_market_features '))\n",
    "#print(con.sql('select * from fact_ohlc_calculated'))\n",
    "con.close()"
   ],
   "id": "f655ef7d58a1508b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "upgrade 2: fit for dashboard chat bot",
   "id": "e8c76addd271fbae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, LongType, TimestampType\n",
    "\n",
    "# --- CONFIG ---\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "\n",
    "# Checkpoint l√† B·∫ÆT BU·ªòC ƒë·ªÉ Spark nh·ªõ tr·∫°ng th√°i (ƒë√£ t√≠nh ƒë·∫øn ƒë√¢u, n·∫øn 5m ƒë√£ gom ƒë∆∞·ª£c bao nhi√™u trade)\n",
    "CHECKPOINT_ROOT = f\"s3a://{S3_BUCKET}/checkpoints/features_streaming_v1/\"\n",
    "WATERMARK_DELAY = \"1 minute\" # ƒê·ªô tr·ªÖ cho ph√©p. Sau 1 ph√∫t k·ªÉ t·ª´ khi n·∫øn ƒë√≥ng, Spark s·∫Ω ghi file.\n",
    "\n",
    "def get_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Streaming_Feature_Engineering\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .getOrCreate()\n",
    "        # T·ªëi ∆∞u cho stateful streaming\n",
    "\n",
    "\n",
    "def get_duckdb_conn():\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "             sql_script = f.read()\n",
    "    con.execute(sql_script)\n",
    "    return con\n",
    "\n",
    "# --- LOGIC T√çNH TO√ÅN AGGREGATION (D√πng chung cho c√°c interval) ---\n",
    "def build_aggregation_plan(df_stream, interval_name, interval_window):\n",
    "    # 1. T√≠nh Turnover ƒë·ªÉ t√≠nh VWAP\n",
    "    df = df_stream.withColumn(\"turnover\", F.col(\"price\") * F.col(\"quantity\"))\n",
    "\n",
    "    # 2. Group By Window & Aggregate\n",
    "    # withWatermark l√† b·∫Øt bu·ªôc ƒë·ªÉ d√πng Append mode\n",
    "    agg_df = df \\\n",
    "        .withWatermark(\"trade_time\", WATERMARK_DELAY) \\\n",
    "        .groupBy(\n",
    "            F.col(\"symbol\"),\n",
    "            F.window(F.col(\"trade_time\"), interval_window).alias(\"window\")\n",
    "        ).agg(\n",
    "            # Basic OHLC\n",
    "            F.min(F.struct(\"trade_time\", \"price\")).getItem(\"price\").alias(\"open\"),\n",
    "            F.max(\"price\").alias(\"high\"),\n",
    "            F.min(\"price\").alias(\"low\"),\n",
    "            F.max(F.struct(\"trade_time\", \"price\")).getItem(\"price\").alias(\"close\"),\n",
    "\n",
    "            # Volume & Counts\n",
    "            F.sum(\"quantity\").alias(\"volume\"),\n",
    "            F.sum(\"turnover\").alias(\"total_turnover\"),\n",
    "            F.count(\"*\").alias(\"trade_count\"),\n",
    "\n",
    "            # Buy/Sell Pressure\n",
    "            F.sum(F.when(F.col(\"side\") == \"buy\", F.col(\"quantity\")).otherwise(0)).alias(\"vol_buy\"),\n",
    "            F.sum(F.when(F.col(\"side\") == \"sell\", F.col(\"quantity\")).otherwise(0)).alias(\"vol_sell\"),\n",
    "\n",
    "            # Statistics (Approximate algorithms for Streaming efficiency)\n",
    "            # L∆∞u √Ω: Streaming kh√¥ng h·ªó tr·ª£ percentile ch√≠nh x√°c ho√†n to√†n, d√πng approx\n",
    "            F.stddev(\"price\").alias(\"price_std\"),\n",
    "            F.skewness(\"price\").alias(\"price_skew\"),\n",
    "            F.kurtosis(\"price\").alias(\"price_kurtosis\"),\n",
    "            F.percentile_approx(\"price\", 0.25).alias(\"price_q25\"),\n",
    "            F.percentile_approx(\"price\", 0.50).alias(\"price_median\"),\n",
    "            F.percentile_approx(\"price\", 0.75).alias(\"price_q75\"),\n",
    "\n",
    "            # Size stats\n",
    "            F.max(\"quantity\").alias(\"size_max\"),\n",
    "            F.avg(\"quantity\").alias(\"size_mean\"),\n",
    "            F.stddev(\"quantity\").alias(\"size_std\")\n",
    "        )\n",
    "\n",
    "    # 3. Final Selection\n",
    "    return agg_df.select(\n",
    "        F.col(\"symbol\"),\n",
    "        F.col(\"window.start\").alias(\"candle_time\"),\n",
    "        F.lit(interval_name).alias(\"interval\"),\n",
    "\n",
    "        \"open\", \"high\", \"low\", \"close\", \"volume\", \"trade_count\",\n",
    "        \"vol_buy\", \"vol_sell\",\n",
    "        (F.col(\"total_turnover\") / F.col(\"volume\")).alias(\"vwap\"),\n",
    "\n",
    "        \"price_std\", \"price_skew\", \"price_kurtosis\",\n",
    "        \"price_q25\", \"price_median\", \"price_q75\",\n",
    "        \"size_max\", \"size_mean\", \"size_std\",\n",
    "\n",
    "        # Partition cho Hive\n",
    "        F.date_format(F.col(\"window.start\"), \"yyyy-MM-dd\").alias(\"date_part\")\n",
    "    )\n",
    "\n",
    "# --- SPARK STREAMING RUNNER ---\n",
    "def process_stream(spark, interval_name, interval_window):\n",
    "    print(f\"\\nüöÄ SPARK STREAM: Processing {interval_name}...\")\n",
    "\n",
    "    # 1. Input: Read Stream t·ª´ Silver Trades\n",
    "    input_path = f\"s3a://{S3_BUCKET}/silver/trades/\"\n",
    "    try:\n",
    "        # L·∫•y schema t·ª´ file m·∫´u ƒë·ªÉ Spark Stream bi·∫øt c·∫•u tr√∫c\n",
    "        schema = spark.read.parquet(input_path).schema\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è No trades data found.\")\n",
    "        return\n",
    "\n",
    "    df_trades = spark.readStream \\\n",
    "        .schema(schema) \\\n",
    "        .format(\"parquet\") \\\n",
    "        .option(\"maxFilesPerTrigger\", 1000) \\\n",
    "        .load(input_path)\n",
    "\n",
    "    # 2. Transform\n",
    "    final_df = build_aggregation_plan(df_trades, interval_name, interval_window)\n",
    "\n",
    "    # 3. Output: Write Stream (Append Mode)\n",
    "    # Ghi v√†o th∆∞ m·ª•c ri√™ng: silver/features/{interval_name}/\n",
    "    output_path = f\"s3a://{S3_BUCKET}/silver/features/{interval_name}\"\n",
    "    checkpoint_path = f\"{CHECKPOINT_ROOT}/{interval_name}\"\n",
    "\n",
    "    query = final_df.writeStream \\\n",
    "        .format(\"parquet\") \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .option(\"checkpointLocation\", checkpoint_path) \\\n",
    "        .trigger(availableNow=True) \\\n",
    "        .partitionBy(\"date_part\") \\\n",
    "        .start(output_path)\n",
    "\n",
    "    # AvailableNow: X·ª≠ l√Ω h·∫øt d·ªØ li·ªáu m·ªõi r·ªìi d·ª´ng -> Ph√π h·ª£p ƒë·ªÉ trigger ƒë·ªãnh k·ª≥ ho·∫∑c v√≤ng l·∫∑p\n",
    "    query.awaitTermination()\n",
    "    print(f\"   ‚úÖ Spark {interval_name}: Done writing batch.\")\n",
    "\n",
    "# --- DUCKDB LOADER (INCREMENTAL) ---\n",
    "def load_to_duckdb(interval_name):\n",
    "    print(f\"ü¶Ü [DuckDB] Syncing {interval_name} features...\")\n",
    "    con = get_duckdb_conn()\n",
    "\n",
    "    # Path ƒë·ªçc file Parquet (Recursive)\n",
    "    parquet_source = f\"s3://{S3_BUCKET}/silver/features/{interval_name}/**/*.parquet\"\n",
    "\n",
    "    try:\n",
    "        # 1. Update Dim Time (T·ª± ƒë·ªông th√™m ng√†y m·ªõi n·∫øu c√≥)\n",
    "        # L·∫•y max time hi·ªán t·∫°i trong Fact\n",
    "        # Logic: Ch·ªâ load nh·ªØng d√≤ng c√≥ candle_time > max_time trong DB\n",
    "\n",
    "        # Check b·∫£ng t·ªìn t·∫°i ch∆∞a\n",
    "        con.execute(\"CREATE TABLE IF NOT EXISTS fact_market_features AS SELECT * FROM read_parquet('\" + parquet_source + \"', hive_partitioning=1) LIMIT 0\")\n",
    "\n",
    "        # Get Max Time\n",
    "        max_time_query = f\"SELECT COALESCE(MAX(candle_time), '1970-01-01'::TIMESTAMP) FROM fact_market_features WHERE interval = '{interval_name}'\"\n",
    "        max_time = con.execute(max_time_query).fetchone()[0]\n",
    "        print(f\"   ‚ÑπÔ∏è Last time in DB: {max_time}\")\n",
    "\n",
    "        # 2. Update Dim Time (Ch·ªâ v·ªõi c√°c d√≤ng m·ªõi)\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO dim_time\n",
    "            SELECT DISTINCT\n",
    "                CAST(strftime(candle_time, '%Y%m%d') AS INTEGER) as date_key,\n",
    "                CAST(candle_time AS DATE),\n",
    "                EXTRACT(YEAR FROM candle_time), EXTRACT(QUARTER FROM candle_time),\n",
    "                EXTRACT(MONTH FROM candle_time), EXTRACT(DAY FROM candle_time),\n",
    "                ISODOW(candle_time), CASE WHEN ISODOW(candle_time) IN (6, 7) THEN TRUE ELSE FALSE END\n",
    "            FROM read_parquet('{parquet_source}', hive_partitioning=1)\n",
    "            WHERE candle_time > '{max_time}'\n",
    "            AND CAST(strftime(candle_time, '%Y%m%d') AS INTEGER) NOT IN (SELECT date_key FROM dim_time)\n",
    "        \"\"\")\n",
    "\n",
    "        # 3. Insert Fact (Ch·ªâ d√≤ng m·ªõi)\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO fact_market_features (\n",
    "                symbol_key, date_key, interval, candle_time,\n",
    "                open, high, low, close, volume, trade_count,\n",
    "                vol_buy, vol_sell, vwap,\n",
    "                price_std, price_skew, price_kurtosis,\n",
    "                price_q25, price_median, price_q75,\n",
    "                size_max, size_mean, size_std\n",
    "            )\n",
    "            SELECT\n",
    "                d.symbol_key,\n",
    "                CAST(strftime(s.candle_time, '%Y%m%d') AS INTEGER) as date_key,\n",
    "                s.interval, s.candle_time,\n",
    "                s.open, s.high, s.low, s.close, s.volume, s.trade_count,\n",
    "                s.vol_buy, s.vol_sell, s.vwap,\n",
    "                s.price_std, s.price_skew, s.price_kurtosis,\n",
    "                s.price_q25, s.price_median, s.price_q75,\n",
    "                s.size_max, s.size_mean, s.size_std\n",
    "            FROM read_parquet('{parquet_source}', hive_partitioning=1) s\n",
    "            JOIN dim_symbol d ON s.symbol = d.symbol_code\n",
    "            WHERE s.candle_time > '{max_time}'\n",
    "        \"\"\")\n",
    "        print(\"   ‚úÖ DuckDB Sync Success.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è DuckDB Error: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "# --- MAIN LOOP ---\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    # ƒê·ªãnh nghƒ©a c√°c khung th·ªùi gian c·∫ßn theo d√µi\n",
    "    intervals = [\n",
    "        (\"1m\", \"1 minute\"),\n",
    "        (\"5m\", \"5 minutes\"),\n",
    "        # (\"1h\", \"1 hour\")\n",
    "    ]\n",
    "\n",
    "    print(\"üöÄ STARTING CONTINUOUS PIPELINE (Ctrl+C to stop)\")\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            start_time = time.time()\n",
    "\n",
    "            for name, window in intervals:\n",
    "                # 1. Spark x·ª≠ l√Ω d·ªØ li·ªáu m·ªõi (Micro-batch)\n",
    "                process_stream(spark, name, window)\n",
    "\n",
    "                # 2. DuckDB n·∫°p d·ªØ li·ªáu m·ªõi v√†o Warehouse\n",
    "                load_to_duckdb(name)\n",
    "\n",
    "            # Ngh·ªâ m·ªôt ch√∫t tr∆∞·ªõc khi qu√©t ti·∫øp (tr√°nh spam CPU n·∫øu kh√¥ng c√≥ data m·ªõi)\n",
    "            # V·ªõi c·∫•u h√¨nh AvailableNow, n·∫øu kh√¥ng c√≥ data m·ªõi, Spark ch·∫°y r·∫•t nhanh r·ªìi tho√°t.\n",
    "            sleep_time = 60\n",
    "            print(f\"üí§ Sleeping {sleep_time}s before next micro-batch...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nüõë Pipeline Stopped.\")\n",
    "        spark.stop()"
   ],
   "id": "1caa00fa91feafdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "order book\n",
   "id": "15368d2db2f34ca8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# --- CONFIG ---\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "STAGING_OUTPUT_PATH = f\"s3a://{S3_BUCKET}/silver/agg_orderbook/\"\n",
    "\n",
    "def get_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Orderbook_Feature_Engineering\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def parse_interval_to_minutes(interval_str):\n",
    "    unit = interval_str[-1].lower()\n",
    "    try:\n",
    "        value = int(interval_str[:-1])\n",
    "    except:\n",
    "        return 60\n",
    "    if unit == 'm': return value\n",
    "    if unit == 'h': return value * 60\n",
    "    if unit == 'd': return value * 1440\n",
    "    return 60\n",
    "\n",
    "def get_last_processed_time(interval_name):\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    try:\n",
    "        table_exists = con.execute(\"SELECT count(*) FROM information_schema.tables WHERE table_name = 'fact_orderbook_features'\").fetchone()[0]\n",
    "        if table_exists == 0: return None\n",
    "\n",
    "        query = f\"SELECT MAX(candle_time) FROM fact_orderbook_features WHERE interval = '{interval_name}'\"\n",
    "        result = con.execute(query).fetchone()[0]\n",
    "        return result\n",
    "    except:\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "def calculate_orderbook_features(spark, interval_name, interval_window):\n",
    "    print(f\"\\nüìö Processing Orderbook: {interval_name} ({interval_window})\")\n",
    "    interval_minutes = parse_interval_to_minutes(interval_name)\n",
    "    buffer_minutes = (interval_minutes * 2) + 10\n",
    "    last_time = get_last_processed_time(interval_name)\n",
    "    # Cutoff Time: Th·ªùi gian hi·ªán t·∫°i tr·ª´ 1 ph√∫t (ƒë·ªÉ ch·∫Øc ch·∫Øn data ƒë√£ v·ªÅ)\n",
    "    # Ch·ªâ x·ª≠ l√Ω c√°c window k·∫øt th√∫c TR∆Ø·ªöC th·ªùi ƒëi·ªÉm n√†y\n",
    "    cutoff_time = datetime.now() - timedelta(minutes=1)\n",
    "\n",
    "    input_path = f\"s3a://{S3_BUCKET}/silver/order_books/\"\n",
    "    df = spark.read.parquet(input_path)\n",
    "\n",
    "    if last_time:\n",
    "        start_timestamp = last_time - timedelta(minutes=buffer_minutes)\n",
    "        start_date_str = start_timestamp.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        print(f\"   ‚ÑπÔ∏è Mode: Incremental (Reading >= {start_timestamp})\")\n",
    "\n",
    "        # Partition Pruning (L·ªçc Folder)\n",
    "        df = df.filter(F.col(\"date_part\") >= F.lit(start_date_str))\n",
    "        # Row Filtering (L·ªçc Time)\n",
    "        df = df.filter(F.col(\"snapshot_time\") >= F.lit(start_timestamp))\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è Mode: Full Load\")\n",
    "    try:\n",
    "        # L·∫•y d√≤ng c√≥ th·ªùi gian nh·ªè nh·∫•t trong batch hi·ªán t·∫°i\n",
    "        min_row = df.select(F.min(\"snapshot_time\")).collect()\n",
    "        min_data_time = min_row[0][0]\n",
    "\n",
    "        if min_data_time is None:\n",
    "            print(\"   ‚ö†Ô∏è No data found in this range.\")\n",
    "            return False\n",
    "\n",
    "        print(f\"   üïí Batch Min Data Time: {min_data_time}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è Error checking min time: {e}\")\n",
    "        return False\n",
    "    # ---------------------------------------------------------\n",
    "    # B∆Ø·ªöC 1: SNAPSHOT AGGREGATION (T√°i t·∫°o s·ªï l·ªánh t·∫°i m·ªói gi√¢y)\n",
    "    # ---------------------------------------------------------\n",
    "    # V√¨ d·ªØ li·ªáu Silver ƒë√£ b·ªã explode (m·ªói d√≤ng 1 m·ª©c gi√°), ta ph·∫£i gom l·∫°i\n",
    "    # ƒë·ªÉ t√≠nh to√°n Best Bid, Best Ask, Total Depth... cho t·ª´ng snapshot_time\n",
    "\n",
    "    snapshot_df = df.groupBy(\"symbol\", \"snapshot_time\").agg(\n",
    "        # Best Price\n",
    "        F.max(F.when(F.col(\"side\") == \"bid\", F.col(\"price\"))).alias(\"best_bid\"),\n",
    "        F.min(F.when(F.col(\"side\") == \"ask\", F.col(\"price\"))).alias(\"best_ask\"),\n",
    "\n",
    "        # Total Volume (Depth)\n",
    "        F.sum(F.when(F.col(\"side\") == \"bid\", F.col(\"quantity\"))).alias(\"sum_bid\"),\n",
    "        F.sum(F.when(F.col(\"side\") == \"ask\", F.col(\"quantity\"))).alias(\"sum_ask\"),\n",
    "\n",
    "        # Weighted Price components (Total Money = Price * Qty)\n",
    "        F.sum(F.col(\"price\") * F.col(\"quantity\")).alias(\"total_turnover\"),\n",
    "        F.sum(\"quantity\").alias(\"total_qty\")\n",
    "    )\n",
    "\n",
    "    # T√≠nh c√°c Derived Features (Gi·ªëng Polars)\n",
    "    # 1. Spread\n",
    "    snapshot_df = snapshot_df.withColumn(\"spread\", F.col(\"best_ask\") - F.col(\"best_bid\"))\n",
    "\n",
    "    # 2. Mid Price\n",
    "    snapshot_df = snapshot_df.withColumn(\"mid_price\", (F.col(\"best_ask\") + F.col(\"best_bid\")) / 2)\n",
    "\n",
    "    # 3. Weighted Mid Price (WMP) ~ total_turnover / total_qty\n",
    "    snapshot_df = snapshot_df.withColumn(\"wmp\", F.col(\"total_turnover\") / F.col(\"total_qty\"))\n",
    "\n",
    "    # 4. Imbalance (Bid / (Bid + Ask))\n",
    "    snapshot_df = snapshot_df.withColumn(\"imbalance\",\n",
    "        F.col(\"sum_bid\") / (F.col(\"sum_bid\") + F.col(\"sum_ask\") + 1e-9)\n",
    "    )\n",
    "\n",
    "    # 5. Book Pressure (WMP - MidPrice)\n",
    "    snapshot_df = snapshot_df.withColumn(\"book_pressure\", F.col(\"wmp\") - F.col(\"mid_price\"))\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # B∆Ø·ªöC 2: WINDOW AGGREGATION (Gom snapshot th√†nh n·∫øn OHLC)\n",
    "    # ---------------------------------------------------------\n",
    "    features_df = snapshot_df.groupBy(\n",
    "        F.col(\"symbol\"),\n",
    "        F.window(F.col(\"snapshot_time\"), interval_window).alias(\"window\")\n",
    "    ).agg(\n",
    "        # --- A. MID PRICE OHLC ---\n",
    "        F.min(F.struct(\"snapshot_time\", \"mid_price\")).getItem(\"mid_price\").alias(\"mid_open\"),\n",
    "        F.max(\"mid_price\").alias(\"mid_high\"),\n",
    "        F.min(\"mid_price\").alias(\"mid_low\"),\n",
    "        F.max(F.struct(\"snapshot_time\", \"mid_price\")).getItem(\"mid_price\").alias(\"mid_close\"),\n",
    "\n",
    "        # --- B. WMP STATS (Weighted Mid Price) ---\n",
    "        F.avg(\"wmp\").alias(\"wmp_mean\"),\n",
    "        F.stddev(\"wmp\").alias(\"wmp_std\"),\n",
    "\n",
    "        # --- C. SPREAD STATS ---\n",
    "        F.avg(\"spread\").alias(\"spread_mean\"),\n",
    "        F.max(\"spread\").alias(\"spread_max\"),\n",
    "\n",
    "        # --- D. IMBALANCE STATS ---\n",
    "        F.avg(\"imbalance\").alias(\"imbal_mean\"),\n",
    "        F.min(\"imbalance\").alias(\"imbal_min\"),\n",
    "        F.max(\"imbalance\").alias(\"imbal_max\"),\n",
    "        F.percentile_approx(\"imbalance\", 0.5).alias(\"imbal_median\"),\n",
    "\n",
    "        # --- E. BOOK PRESSURE ---\n",
    "        F.avg(\"book_pressure\").alias(\"pressure_mean\"),\n",
    "        F.stddev(\"book_pressure\").alias(\"pressure_std\"),\n",
    "\n",
    "        # --- F. DEPTH STATS ---\n",
    "        F.avg(\"sum_bid\").alias(\"depth_bid_mean\"),\n",
    "        F.avg(\"sum_ask\").alias(\"depth_ask_mean\"),\n",
    "\n",
    "        # Count\n",
    "        F.count(\"*\").alias(\"snapshot_count\")\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------------------------\n",
    "    # B∆Ø·ªöC 3: STRICT FILTER (Ch·ªâ l·∫•y n·∫øn ƒë√£ ƒë√≥ng)\n",
    "    # ---------------------------------------------------------\n",
    "    features_df = features_df.filter(\n",
    "        (F.col(\"window.end\") <= F.lit(cutoff_time)) &\n",
    "        (F.col(\"window.start\") >= F.lit(min_data_time))\n",
    "    )\n",
    "    if features_df.rdd.isEmpty():\n",
    "        print(f\"   ‚ö†Ô∏è No closed candles found for {interval_name}. Waiting for data...\")\n",
    "        return False\n",
    "\n",
    "    final_df = features_df.select(\n",
    "        F.col(\"symbol\"),\n",
    "        F.col(\"window.start\").alias(\"candle_time\"),\n",
    "        F.lit(interval_name).alias(\"interval\"),\n",
    "\n",
    "        \"mid_open\", \"mid_high\", \"mid_low\", \"mid_close\",\n",
    "        \"wmp_mean\", \"wmp_std\",\n",
    "        \"spread_mean\", \"spread_max\",\n",
    "        \"imbal_mean\", \"imbal_min\", \"imbal_max\", \"imbal_median\",\n",
    "        \"pressure_mean\", \"pressure_std\",\n",
    "        \"depth_bid_mean\", \"depth_ask_mean\",\n",
    "        \"snapshot_count\",\n",
    "        F.current_timestamp().alias(\"ingestion_time\")\n",
    "    )\n",
    "\n",
    "    # 4. WRITE TO STAGING (Overwriting interval specific folder)\n",
    "    staging_path = f\"{STAGING_OUTPUT_PATH}/{interval_name}\"\n",
    "    print(f\"   üíæ Writing to Staging: {staging_path}\")\n",
    "    final_df.write.mode(\"overwrite\").parquet(staging_path)\n",
    "\n",
    "    return True\n",
    "\n",
    "def merge_to_duckdb(interval_name):\n",
    "    print(f\"ü¶Ü Merging Orderbook {interval_name} into DuckDB...\")\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "\n",
    "    # Config MinIO\n",
    "    with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "             sql_script = f.read()\n",
    "    con.execute(sql_script)\n",
    "\n",
    "    staging_source = f\"s3://{S3_BUCKET}/silver/agg_orderbook/{interval_name}/*.parquet\"\n",
    "\n",
    "    try:\n",
    "        # Create Table (Schema to l·ªõn)\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS fact_orderbook_features AS\n",
    "            SELECT * FROM read_parquet('{staging_source}') LIMIT 0\n",
    "        \"\"\")\n",
    "\n",
    "        # Clean overlapping data\n",
    "        print(\"   üîÑ Cleaning overlapping data...\")\n",
    "        con.execute(f\"\"\"\n",
    "            DELETE FROM fact_orderbook_features\n",
    "            WHERE interval = '{interval_name}'\n",
    "            AND (symbol, candle_time) IN (\n",
    "                SELECT symbol, candle_time FROM read_parquet('{staging_source}')\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert new data\n",
    "        print(\"   üì• Inserting new data...\")\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO fact_orderbook_features\n",
    "            SELECT * FROM read_parquet('{staging_source}')\n",
    "        \"\"\")\n",
    "        print(\"   ‚úÖ Merge Complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"No files found\" in str(e):\n",
    "            print(f\"   ‚ÑπÔ∏è No new data for {interval_name}.\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è DuckDB Error: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    intervals = [\n",
    "        (\"1m\", \"1 minute\"),\n",
    "        (\"5m\", \"5 minutes\"),\n",
    "        (\"15m\", \"15 minutes\"),\n",
    "        (\"1h\", \"1 hour\"),\n",
    "        (\"4h\", \"4 hours\")\n",
    "    ]\n",
    "\n",
    "    print(f\"STARTING ORDERBOOK BATCH PIPELINE...\")\n",
    "\n",
    "    for name, window in intervals:\n",
    "        has_data = calculate_orderbook_features(spark, name, window)\n",
    "        if has_data:\n",
    "            merge_to_duckdb(name)\n",
    "\n",
    "    spark.stop()"
   ],
   "id": "f69ca0bcbaeabc86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "insert data tables",
   "id": "727d3f362b331f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import boto3\n",
    "import duckdb\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, to_timestamp, date_format, lit, current_timestamp\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, ArrayType, DoubleType)\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "def get_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"ETL_Silver_Loader\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .getOrCreate()\n",
    "def get_duckdb_conn():\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "         sql_script = f.read()\n",
    "    con.execute(sql_script)\n",
    "    return con\n",
    "def get_schemas():\n",
    "    trade_element = StructType([\n",
    "        StructField(\"instId\", StringType(), True),\n",
    "        StructField(\"tradeId\", StringType(), True),\n",
    "        StructField(\"px\", StringType(), True),\n",
    "        StructField(\"sz\", StringType(), True),\n",
    "        StructField(\"side\", StringType(), True),\n",
    "        StructField(\"ts\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    funding_element = StructType([\n",
    "        StructField(\"instId\", StringType(), True),\n",
    "        StructField(\"fundingRate\", StringType(), True),\n",
    "        StructField(\"nextFundingRate\", StringType(), True),\n",
    "        StructField(\"fundingTime\", StringType(), True),\n",
    "        StructField(\"nextFundingTime\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    ohlc_element = ArrayType(StringType())\n",
    "\n",
    "    book_element = StructType([\n",
    "\n",
    "        StructField(\"asks\", ArrayType(ArrayType(StringType())), True),\n",
    "        StructField(\"bids\", ArrayType(ArrayType(StringType())), True),\n",
    "        #StructField(\"instId\", StringType(), True),\n",
    "        StructField(\"ts\", StringType(), True)\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"trades\": trade_element,\n",
    "        \"funding\": funding_element,\n",
    "        \"ohlc\": ohlc_element,\n",
    "        \"book\": book_element\n",
    "    }\n",
    "def get_raw_schema(data_schema):\n",
    "    return StructType([\n",
    "        StructField(\"received_at\", StringType(), True),\n",
    "        StructField(\"payload\", StructType([\n",
    "            StructField(\"data\", ArrayType(data_schema), True),\n",
    "            StructField(\"arg\", StructType([\n",
    "                StructField(\"channel\", StringType(), True),\n",
    "                StructField(\"instId\", StringType(), True)\n",
    "            ]), True)\n",
    "        ]), True)\n",
    "    ])\n",
    "\n",
    "def transform_trades(df):\n",
    "    return df.select(\n",
    "        col(\"received_at\"), explode(col(\"payload.data\")).alias(\"data\")\n",
    "    ).select(\n",
    "        col(\"data.instId\").alias(\"symbol\"),\n",
    "        col(\"data.tradeId\").alias(\"tradeId\"),\n",
    "        col(\"data.side\").alias(\"side\"),\n",
    "        col(\"data.px\").cast(\"double\").alias(\"price\"),\n",
    "        col(\"data.sz\").cast(\"double\").alias(\"quantity\"),\n",
    "        (col(\"data.ts\").cast(\"long\") / 1000).cast(\"timestamp\").alias(\"trade_time\"),\n",
    "        col(\"received_at\").cast(\"timestamp\").alias(\"ingestion_time\")\n",
    "    ).dropDuplicates([\"tradeId\"])\n",
    "def transform_funding(df_raw):\n",
    "    return df_raw.select(\n",
    "        col(\"received_at\"), explode(col(\"payload.data\")).alias(\"data\")\n",
    "    ).select(\n",
    "        col(\"data.instId\").alias(\"symbol\"),\n",
    "        lit(\"SWAP\").alias(\"instrument_type\"), # Funding th∆∞·ªùng l√† SWAP\n",
    "        col(\"data.fundingRate\").cast(\"double\").alias(\"funding_rate\"),\n",
    "        col(\"data.nextFundingRate\").cast(\"double\").alias(\"next_funding_rate\"),\n",
    "        (col(\"data.fundingTime\").cast(\"long\") / 1000).cast(\"timestamp\").alias(\"funding_time\"),\n",
    "        (col(\"data.nextFundingTime\").cast(\"long\") / 1000).cast(\"timestamp\").alias(\"next_funding_time\"),\n",
    "        col(\"received_at\").cast(\"timestamp\").alias(\"ingestion_time\")\n",
    "    )\n",
    "def transform_ohlc(df_raw, candle_type=\"mark\"):\n",
    "    # Index mapping: 0:ts, 1:o, 2:h, 3:l, 4:c\n",
    "    return df_raw.select(\n",
    "        col(\"received_at\"),\n",
    "        col(\"payload.arg.instId\").alias(\"symbol\"),\n",
    "        col(\"payload.arg.channel\").alias(\"channel\"),\n",
    "        explode(col(\"payload.data\")).alias(\"c\")\n",
    "    ).select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"channel\"),\n",
    "        (col(\"c\")[0].cast(\"long\") / 1000).cast(\"timestamp\").alias(\"candle_time\"),\n",
    "        col(\"c\")[1].cast(\"double\").alias(\"open\"),\n",
    "        col(\"c\")[2].cast(\"double\").alias(\"high\"),\n",
    "        col(\"c\")[3].cast(\"double\").alias(\"low\"),\n",
    "        col(\"c\")[4].cast(\"double\").alias(\"close\"),\n",
    "        col(\"received_at\").cast(\"timestamp\").alias(\"ingestion_time\")\n",
    "        #lit(candle_type).alias(\"type\")\n",
    "    )\n",
    "def transform_orderbook(df_raw):\n",
    "    # Logic ph·ª©c t·∫°p cho orderbook (Explode Asks/Bids -> Union)\n",
    "    df_exploded = df_raw.select(\n",
    "        col(\"received_at\"),\n",
    "        col(\"payload.arg.instId\").alias(\"symbol\"), # <--- L·∫§Y T·ª™ ARG\n",
    "        explode(col(\"payload.data\")).alias(\"book\")\n",
    "    )\n",
    "    # Process Asks\n",
    "    df_asks = df_exploded.select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"book.ts\").alias(\"ts\"),\n",
    "        col(\"received_at\"),  # Gi·ªØ l·∫°i c·ªôt n√†y\n",
    "        explode(col(\"book.asks\")).alias(\"asks\"),\n",
    "        lit(\"ask\").alias(\"side\")\n",
    "    ).select(\n",
    "        col(\"symbol\"), col(\"side\"),\n",
    "        col(\"asks\")[0].cast(\"double\").alias(\"price\"),\n",
    "        col(\"asks\")[1].cast(\"double\").alias(\"quantity\"),\n",
    "        col(\"ts\"), col(\"received_at\")\n",
    "    )\n",
    "\n",
    "    # Process Bids\n",
    "    df_bids = df_exploded.select(\n",
    "        col(\"symbol\"),\n",
    "        col(\"book.ts\").alias(\"ts\"),\n",
    "        col(\"received_at\"),  # Gi·ªØ l·∫°i c·ªôt n√†y\n",
    "        explode(col(\"book.bids\")).alias(\"bids\"),\n",
    "        lit(\"bid\").alias(\"side\")\n",
    "    ).select(\n",
    "        col(\"symbol\"), col(\"side\"),\n",
    "        col(\"bids\")[0].cast(\"double\").alias(\"price\"),\n",
    "        col(\"bids\")[1].cast(\"double\").alias(\"quantity\"),\n",
    "        col(\"ts\"), col(\"received_at\")\n",
    "    )\n",
    "\n",
    "    # Union v√† chu·∫©n h√≥a time\n",
    "    return df_asks.union(df_bids) \\\n",
    "        .withColumn(\"snapshot_time\", (col(\"ts\").cast(\"long\") / 1000).cast(\"timestamp\")) \\\n",
    "        .withColumn(\"ingestion_time\", col(\"received_at\").cast(\"timestamp\")) \\\n",
    "        .drop(\"ts\", \"received_at\")\n",
    "    # Quan tr·ªçng: D√≤ng tr√™n ƒë·ªïi t√™n received_at -> ingestion_time\n",
    "\n",
    "def load_gold_layer(con, stg_table):\n",
    "    print(f\"   ‚ú® Loading Gold Layer for {stg_table}...\")\n",
    "\n",
    "    # 1. Update Dim Symbol\n",
    "    con.execute(f\"\"\"\n",
    "        INSERT INTO dim_symbol (symbol_code, base_currency, quote_currency)\n",
    "        SELECT DISTINCT symbol, split_part(symbol, '-', 1), split_part(symbol, '-', 2)\n",
    "        FROM {stg_table}\n",
    "        WHERE symbol NOT IN (SELECT symbol_code FROM dim_symbol)\n",
    "    \"\"\")\n",
    "\n",
    "    time_col_map = {\n",
    "        \"trades\": \"trade_time\",\n",
    "        \"funding_rate\": \"funding_time\",\n",
    "        \"ohlc_mark\": \"candle_time\",\n",
    "        \"order_books\": \"snapshot_time\"\n",
    "    }\n",
    "    t_col = time_col_map.get(stg_table, \"ingestion_time\")\n",
    "\n",
    "    con.execute(f\"\"\"\n",
    "        INSERT INTO dim_time\n",
    "        SELECT DISTINCT\n",
    "            CAST(strftime({t_col}, '%Y%m%d') AS INTEGER) as date_key,\n",
    "            CAST({t_col} AS DATE),\n",
    "            EXTRACT(YEAR FROM {t_col}), EXTRACT(QUARTER FROM {t_col}),\n",
    "            EXTRACT(MONTH FROM {t_col}), EXTRACT(DAY FROM {t_col}),\n",
    "            ISODOW({t_col}), CASE WHEN ISODOW({t_col}) IN (6, 7) THEN TRUE ELSE FALSE END\n",
    "        FROM {stg_table}\n",
    "        WHERE CAST(strftime({t_col}, '%Y%m%d') AS INTEGER) NOT IN (SELECT date_key FROM dim_time)\n",
    "    \"\"\")\n",
    "\n",
    "    # 3. Insert Fact Tables\n",
    "    if stg_table == \"trades\":\n",
    "        con.execute(\"\"\"\n",
    "                    INSERT INTO fact_trades (symbol_key, date_key, trade_id, price, quantity, trade_time)\n",
    "                    SELECT d.symbol_key,\n",
    "                           CAST(strftime(s.trade_time, '%Y%m%d') AS INTEGER),\n",
    "                           s.tradeId,\n",
    "                           s.price,\n",
    "                           s.quantity,\n",
    "                           s.trade_time\n",
    "                    FROM trades s\n",
    "                             JOIN dim_symbol d ON s.symbol = d.symbol_code\n",
    "                    WHERE s.trade_time > (SELECT COALESCE(MAX(trade_time), '1970-01-01'::TIMESTAMP) FROM fact_trades)\n",
    "                    \"\"\")\n",
    "    elif stg_table == \"funding_rate\":\n",
    "        con.execute(\"\"\"\n",
    "                    INSERT INTO fact_funding_rate (symbol_key, date_key, funding_rate, next_funding_rate, funding_time)\n",
    "                    SELECT d.symbol_key,\n",
    "                           CAST(strftime(s.funding_time, '%Y%m%d') AS INTEGER),\n",
    "                           s.funding_rate,\n",
    "                           s.next_funding_rate,\n",
    "                           s.funding_time\n",
    "                    FROM funding_rate s\n",
    "                             JOIN dim_symbol d ON s.symbol = d.symbol_code\n",
    "                    WHERE s.funding_time >\n",
    "                          (SELECT COALESCE(MAX(funding_time), '1970-01-01'::TIMESTAMP) FROM fact_funding_rate)\n",
    "                    \"\"\")\n",
    "\n",
    "    print(\"   ‚úÖ Gold Load Success\")"
   ],
   "id": "cbd1dfa9623295bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "spark=get_spark()\n",
    "config_key='book'\n",
    "bronze_prefix=\"bronze/okx_orderbook/\"\n",
    "stg_table= \"order_books\"\n",
    "#transform_orderbook_func=transform_orderbook()\n",
    "data_schema=get_schemas()[\"book\"]\n",
    "print(f\"\\nüöÄ Processing: {config_key} -> {stg_table}\")\n",
    "\n",
    "df_raw = spark.read.schema(get_raw_schema(data_schema)).json(f\"s3a://{S3_BUCKET}/{bronze_prefix}*/*/*.jsonl.gz\")\n",
    "    # 2. Transform\n",
    "df_silver = transform_orderbook(df_raw)\n",
    "time_col = None\n",
    "for c in [\"trade_time\", \"funding_time\", \"candle_time\", \"snapshot_time\"]:\n",
    "    if c in df_silver.columns: time_col = col(c); break\n",
    "df_silver = df_silver.withColumn(\n",
    "    \"date_part\",\n",
    "    date_format(time_col if time_col is not None else current_timestamp(), \"yyyy-MM-dd\"))\n",
    "#df_silver.show()\n",
    "silver_path = f\"s3a://{S3_BUCKET}/silver/{stg_table}/\"\n",
    "df_silver.write.mode(\"append\").partitionBy(\"date_part\").format(\"parquet\").save(silver_path)"
   ],
   "id": "36d796083b9d08e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "index-price",
   "id": "e122687eda0fcdbe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "# --- CONFIG ---\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "STAGING_OUTPUT_PATH = f\"s3a://{S3_BUCKET}/silver/indexPriceKlines/\"\n",
    "def get_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"IndexPriceKlines_Aggregator\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def parse_interval_to_minutes(interval_str):\n",
    "    unit = interval_str[-1].lower()\n",
    "    try:\n",
    "        value = int(interval_str[:-1])\n",
    "    except:\n",
    "        return 60\n",
    "    if unit == 'm': return value\n",
    "    if unit == 'h': return value * 60\n",
    "    if unit == 'd': return value * 1440\n",
    "    return 60\n",
    "\n",
    "def parse_interval_to_ms(interval_str):\n",
    "    \"\"\"ƒê·ªïi sang ms\"\"\"\n",
    "    return parse_interval_to_minutes(interval_str) * 60 * 1000\n",
    "\n",
    "def get_last_processed_time(interval_name):\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    try:\n",
    "        table_exists = con.execute(\"SELECT count(*) FROM information_schema.tables WHERE table_name = 'fact_indexpriceklines_features'\").fetchone()[0]\n",
    "        if table_exists == 0: return None\n",
    "        query = f\"SELECT MAX(close_time) FROM fact_indexpriceklines_features WHERE interval = '{interval_name}'\"\n",
    "        result = con.execute(query).fetchone()[0]\n",
    "        return result\n",
    "    except:\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()\n",
    "def calculate_indexpriceklines_features(spark, interval_name, interval_window_ms):\n",
    "    print(f\"\\nüìà Processing IndexPriceKlines: {interval_name} (window: {interval_window_ms / 1000 / 60} minutes)\")\n",
    "    # 1. SETUP TH·ªúI GIAN (Incremental + Buffer)\n",
    "    interval_ms = parse_interval_to_ms(interval_name)\n",
    "    buffer_ms = interval_ms * 2  # Buffer g·∫•p ƒë√¥i interval ƒë·ªÉ an to√†n\n",
    "    last_time = get_last_processed_time(interval_name)\n",
    "    cutoff_time = datetime.now() - timedelta(minutes=1)  # Kh√¥ng l·∫•y n·∫øn hi·ªán t·∫°i\n",
    "    input_path = f\"s3a://{S3_BUCKET}/silver/ohlc_index/\"\n",
    "    # ƒê·ªçc d·ªØ li·ªáu t·ª´ MinIO bronze (ƒë√£ n√©n jsonl.gz, partitioned date/hour)\n",
    "    df = spark.read.parquet(input_path)\n",
    "\n",
    "    df = df.filter(F.col(\"channel\") == f\"index-candle{interval_name}\")\n",
    "\n",
    "    # Transform raw th√†nh 1m klines (gi·ªëng ETL)\n",
    "    df = df.withColumnRenamed(\"candle_time\", \"close_time\")\n",
    "    # Gi·∫£ ƒë·ªãnh time_col l√† close_time\n",
    "    time_col = col(\"close_time\")\n",
    "    if last_time:\n",
    "        start_timestamp = last_time - timedelta(milliseconds=buffer_ms)\n",
    "        start_date_str = start_timestamp.strftime(\"%Y-%m-%d\")\n",
    "        prev_date_str = (start_timestamp - timedelta(days=1)).strftime(\"%Y-%m-%d\")\n",
    "        print(f\" ‚ÑπÔ∏è Incremental Mode: Reading >= {start_timestamp}, including prev day {prev_date_str}\")\n",
    "        # Filter theo th·ªùi gian (Spark s·∫Ω pruning n·∫øu partitioned ƒë√∫ng)\n",
    "        df = df.filter(time_col >= F.lit(start_timestamp))\n",
    "    else:\n",
    "        print(\" ‚ÑπÔ∏è Mode: Full Load\")\n",
    "    # 2. CHECK MIN DATA TIME (ƒê·ªÉ ch·∫∑n n·∫øn thi·∫øu ƒë·∫ßu - gi·ªëng Polars strict filter)\n",
    "    try:\n",
    "        min_row = df.select(F.min(time_col).alias(\"min_time\")).collect()[0]\n",
    "        min_data_time = min_row[\"min_time\"]\n",
    "        if min_data_time is None:\n",
    "            print(\" ‚ö†Ô∏è No data found in this range.\")\n",
    "            return False\n",
    "        print(f\" üïí Batch Min Data Time: {min_data_time}\")\n",
    "    except Exception as e:\n",
    "        print(f\" ‚ö†Ô∏è Error checking min time: {e}\")\n",
    "        return False\n",
    "    # 3. WINDOW AGGREGATION (Resample gi·ªëng Polars)\n",
    "    interval_minutes = int(interval_ms / 1000 / 60)\n",
    "    interval_window = f\"{interval_minutes} minutes\"\n",
    "    features_df = df.groupBy(\n",
    "        F.col(\"symbol\"),\n",
    "        F.window(time_col.cast(TimestampType()), interval_window).alias(\"window\")\n",
    "    ).agg(\n",
    "        # OHLC - gi·ªëng h·ªát Polars\n",
    "        F.first(\"open\").alias(\"open\"),  # First open in window\n",
    "        F.max(\"high\").alias(\"high\"),\n",
    "        F.min(\"low\").alias(\"low\"),\n",
    "        F.last(\"close\").alias(\"close\"),  # Last close in window\n",
    "\n",
    "        # Stats - gi·ªëng Polars\n",
    "        F.avg(\"close\").alias(\"mean\"),\n",
    "        F.stddev(\"close\").alias(\"std\")\n",
    "    )\n",
    "\n",
    "    # 4. STRICT FILTER (Closed candles only - gi·ªëng Polars)\n",
    "#    cutoff_ms = cutoff_time.timestamp() * 1000\n",
    " #   min_data_ms = min_data_time if isinstance(min_data_time, int) else min_data_time.timestamp() * 1000\n",
    "    cutoff_timestamp = cutoff_time\n",
    "    features_df = features_df.filter(\n",
    "        (F.col(\"window.end\") <= F.lit(cutoff_timestamp)) &\n",
    "        (F.col(\"window.start\") >= F.lit(min_data_time))\n",
    "    )\n",
    "\n",
    "    if features_df.rdd.isEmpty():\n",
    "        print(f\" ‚ö†Ô∏è No closed candles found for {interval_name}.\")\n",
    "        return False\n",
    "\n",
    "    # Th√™m metadata v√† partition column\n",
    "    final_df = features_df.select(\n",
    "        F.col(\"symbol\"),\n",
    "        F.col(\"window.end\").alias(\"close_time\"),  # S·ª≠ d·ª•ng end l√†m timestamp_dt\n",
    "        F.lit(interval_name).alias(\"interval\"),\n",
    "        #F.col('channel').alias(\"interval\"),\n",
    "        \"open\", \"high\", \"low\", \"close\", \"mean\", \"std\",\n",
    "        F.current_timestamp().alias(\"ingestion_time\")\n",
    "    ).withColumn(\n",
    "        \"date_part\",\n",
    "        F.date_format(F.col(\"close_time\"), \"yyyy-MM-dd\")\n",
    "    )\n",
    "\n",
    "    # 5. WRITE TO STAGING (MinIO) - partitioned by date_part gi·ªëng silver ETL\n",
    "    staging_path = f\"{STAGING_OUTPUT_PATH}/{interval_name}\"\n",
    "    print(f\" üíæ Writing to Staging: {staging_path}\")\n",
    "    final_df.write.mode(\"append\").partitionBy(\"date_part\").parquet(staging_path)\n",
    "    return True\n",
    "\n",
    "def merge_to_duckdb(interval_name):\n",
    "    print(f\"ü¶Ü Merging IndexPriceKlines {interval_name} into DuckDB...\")\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "\n",
    "    # Config MinIO cho DuckDB\n",
    "    with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "            sql_script = f.read()\n",
    "    con.execute(sql_script)\n",
    "\n",
    "    staging_source = f\"s3://{S3_BUCKET}/silver/indexPriceKlines/{interval_name}/*/*.parquet\"  # ƒê·ªçc partitioned parquet\n",
    "\n",
    "    try:\n",
    "        # Create Fact Table n·∫øu ch∆∞a c√≥\n",
    "        con.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS fact_indexpriceklines_features (\n",
    "                symbol VARCHAR,\n",
    "                close_time TIMESTAMP,\n",
    "                interval VARCHAR,\n",
    "                open DOUBLE, high DOUBLE, low DOUBLE, close DOUBLE,\n",
    "                mean DOUBLE, std DOUBLE,\n",
    "                ingestion_time TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Upsert Logic (Delete old + Insert new) - gi·ªëng incremental Polars\n",
    "        print(\" üîÑ Cleaning overlapping data...\")\n",
    "        con.execute(f\"\"\"\n",
    "            DELETE FROM fact_indexpriceklines_features\n",
    "            WHERE interval = '{interval_name}'\n",
    "            AND (symbol, close_time) IN (\n",
    "                SELECT symbol, close_time FROM read_parquet('{staging_source}', hive_partitioning=1)\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        print(\" üì• Inserting new data...\")\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO fact_indexpriceklines_features\n",
    "            SELECT\n",
    "                symbol,\n",
    "                close_time,\n",
    "                interval,\n",
    "                open, high, low, close,\n",
    "                mean, std,\n",
    "                ingestion_time\n",
    "            FROM read_parquet('{staging_source}', hive_partitioning=1)\n",
    "        \"\"\")\n",
    "        print(\" ‚úÖ Merge Complete.\")\n",
    "    except Exception as e:\n",
    "        if \"No files found\" in str(e):\n",
    "            print(f\" ‚ÑπÔ∏è No new data for {interval_name}.\")\n",
    "        else:\n",
    "            print(f\" ‚ö†Ô∏è DuckDB Error: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = get_spark()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    intervals = [\n",
    "        (\"1m\", parse_interval_to_ms(\"1m\")),\n",
    "        (\"5m\", parse_interval_to_ms(\"5m\")),\n",
    "        (\"15m\", parse_interval_to_ms(\"15m\")),\n",
    "        (\"1h\", parse_interval_to_ms(\"1h\")),\n",
    "        (\"4h\", parse_interval_to_ms(\"4h\")),\n",
    "        (\"1d\", parse_interval_to_ms(\"1d\"))\n",
    "    ]\n",
    "\n",
    "    print(\"STARTING INDEXPRICEKLINES BATCH PIPELINE...\")\n",
    "    for name, window_ms in intervals:\n",
    "        has_data = calculate_indexpriceklines_features(spark, name, window_ms)\n",
    "        if has_data:\n",
    "            merge_to_duckdb(name)\n",
    "\n",
    "    spark.stop()"
   ],
   "id": "b457c461ac6e452d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "mark-price",
   "id": "d84ff926a94d1330"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "# --- CONFIG ---\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "STAGING_OUTPUT_PATH = f\"s3a://{S3_BUCKET}/silver/markindex/\"\n",
    "def get_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"IndexPriceKlines_Aggregator\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "def parse_interval_to_minutes(interval_str):\n",
    "    unit = interval_str[-1].lower()\n",
    "    try:\n",
    "        value = int(interval_str[:-1])\n",
    "    except:\n",
    "        return 60\n",
    "    if unit == 'm': return value\n",
    "    if unit == 'h': return value * 60\n",
    "    if unit == 'd': return value * 1440\n",
    "    return 60\n",
    "\n",
    "def parse_interval_to_ms(interval_str):\n",
    "    \"\"\"ƒê·ªïi sang ms\"\"\"\n",
    "    return parse_interval_to_minutes(interval_str) * 60 * 1000\n",
    "\n",
    "def get_last_processed_time(interval_name):\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    try:\n",
    "        table_exists = con.execute(\"SELECT count(*) FROM information_schema.tables WHERE table_name = 'fact_indexpriceklines_features'\").fetchone()[0]\n",
    "        if table_exists == 0: return None\n",
    "        query = f\"SELECT MAX(close_time) FROM fact_indexpriceklines_features WHERE interval = '{interval_name}'\"\n",
    "        result = con.execute(query).fetchone()[0]\n",
    "        return result\n",
    "    except:\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()"
   ],
   "id": "1f84594133b86454",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "path='s3a://trading-okx/silver/ohlc_index/'\n",
    "spark=get_spark()\n",
    "df=spark.read.parquet(path)\n",
    "df.sort(\"ingestion_time\",ascending=False).show(truncate=False)\n",
    "out='s3a://trading-okx/silver/123/'\n",
    "df.coalesce(1).write.mode('overwrite').csv(out)"
   ],
   "id": "45d6d86d2107c91c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "# --- CONFIG ---\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "STAGING_OUTPUT_PATH = f\"s3a://{S3_BUCKET}/silver/markindex/\"\n",
    "\n",
    "\n",
    "def get_spark():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"IndexPriceKlines_Aggregator\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n"
   ],
   "id": "6fe1ce2d4cabdce8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "input_path = f\"s3a://trading-okx/silver/agg_orderbook/*/*\"\n",
    "spark=get_spark()\n",
    "df = spark.read.parquet(input_path)\n",
    "df.show(truncate=False)"
   ],
   "id": "db363635ce21ce49",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType\n",
    "from pyspark.sql.functions import broadcast\n",
    "from functools import reduce\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "PATH_SILVER_BASE = f\"s3a://trading-okx/silver\"\n",
    "DUCKDB_INIT_SCRIPT= '/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql'\n",
    "#SYMBOL= \"btc-usdt-swap\"\n",
    "SYMBOL= \"btc-usdt\"\n",
    "PATH_GOLD_STAGING = \"s3a://trading-okx/gold/staging_merged_features\"\n",
    "PATH_GOLD_FINAL = \"s3a://trading-okx/gold/fact_merged_features\"\n",
    "SOURCE_MAPPING = {\n",
    "    \"trade\": \"agg_trades\",\n",
    "    \"book\": \"agg_orderbook\",\n",
    "    \"index\": \"indexPriceKlines\",\n",
    "    \"mark\": \"markPriceKlines\"\n",
    "}\n",
    "INTERVALS = [\"1m\",\"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "# --- 1. UTILS ---\n",
    "def get_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"MultiTimeframe_Merger\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "def get_last_processed_time(interval):\n",
    "    \"\"\"L·∫•y timestamp cu·ªëi c√πng t·ª´ DuckDB ƒë·ªÉ ch·∫°y Incremental\"\"\"\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    try:\n",
    "        # Load MinIO config\n",
    "        with open(DUCKDB_INIT_SCRIPT, 'r') as f:\n",
    "            con.execute(f.read())\n",
    "\n",
    "        # Check table existence\n",
    "        exists = con.execute(\n",
    "            \"SELECT count(*) FROM information_schema.tables WHERE table_name = 'fact_merged_features'\"\n",
    "        ).fetchone()[0]\n",
    "\n",
    "        if exists == 0: return None\n",
    "\n",
    "        # L·∫•y max time c·ªßa interval t∆∞∆°ng ·ª©ng\n",
    "        query = f\"SELECT MAX(timestamp) FROM fact_merged_features WHERE interval = '{interval}'\"\n",
    "        result = con.execute(query).fetchone()[0]\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è DuckDB Info: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()\n",
    "def load_agg_source(spark, source_alias, table_name, interval, last_time):\n",
    "    # C·∫•u tr√∫c path gi·∫£ ƒë·ªãnh: silver/agg_trade/btc-usdt/5m/\n",
    "    # B·∫°n c·∫ßn ƒë·∫£m b·∫£o c·∫•u tr√∫c folder n√†y kh·ªõp v·ªõi pipeline silver c·ªßa b·∫°n\n",
    "    path = f\"{PATH_SILVER_BASE}/{table_name}/{interval}/\"\n",
    "\n",
    "    try:\n",
    "        df = spark.read.parquet(path + \"*\")\n",
    "    except Exception:\n",
    "        print(f\"   ‚ö†Ô∏è Path not found: {path}\")\n",
    "        return None\n",
    "    if last_time:\n",
    "        # L·∫•y l√πi 2 interval ƒë·ªÉ ƒë·∫£m b·∫£o kh·ªõp bi√™n (Boundary)\n",
    "        buffer_time = last_time - timedelta(hours=1) # Buffer an to√†n\n",
    "        start_date_str = buffer_time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        print(f\"   ‚ÑπÔ∏è Incr Load {source_alias}: >= {buffer_time}\")\n",
    "        df = df.filter(F.col(\"date_part\") >= F.lit(start_date_str))\n",
    "\n",
    "        # X√°c ƒë·ªãnh c·ªôt time (Th∆∞·ªùng c√°c b·∫£ng agg ƒë√£ chu·∫©n h√≥a t√™n c·ªôt time, v√≠ d·ª•: window_end ho·∫∑c timestamp)\n",
    "        # ·ªû ƒë√¢y gi·∫£ ƒë·ªãnh c·ªôt time t√™n l√† 'timestamp' ho·∫∑c 'close_time'\n",
    "        time_col = \"candle_time\" if \"candle_time\" in df.columns else \"close_time\"\n",
    "        df = df.filter(F.col(time_col) >= F.lit(buffer_time))\n",
    "\n",
    "    if df.rdd.isEmpty():\n",
    "        return None\n",
    "\n",
    "    # --- Standardization ---\n",
    "    # 1. Chu·∫©n h√≥a c·ªôt time v·ªÅ 'timestamp'\n",
    "    for t_col in [\"close_time\", \"window_end\", \"candle_time\"]:\n",
    "        if t_col in df.columns:\n",
    "            df = df.withColumnRenamed(t_col, \"timestamp\")\n",
    "            break\n",
    "\n",
    "    df = df.withColumn(\"timestamp\", F.col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "    # 2. ƒê·ªïi t√™n c√°c c·ªôt features (tr·ª´ timestamp v√† c√°c c·ªôt key)\n",
    "    # V√≠ d·ª•: open -> trade_open, close -> trade_close\n",
    "    exclude_cols = [\"timestamp\", \"symbol\", \"interval\", \"date_part\", \"ingestion_time\"]\n",
    "    rename_mapping = {}\n",
    "\n",
    "    for col_name in df.columns:\n",
    "        if col_name not in exclude_cols:\n",
    "            new_name = f\"{source_alias}_{col_name}\"\n",
    "            rename_mapping[col_name] = new_name\n",
    "\n",
    "    for old, new in rename_mapping.items():\n",
    "        df = df.withColumnRenamed(old, new)\n",
    "# Remove Silver metadata columns\n",
    "    for col in [ \"symbol\", \"interval\", \"date_part\", \"ingestion_time\"]:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(col)\n",
    "\n",
    "    return df\n",
    "def load_funding_rate(spark, last_time):\n",
    "    # ƒê∆∞·ªùng d·∫´n funding rate\n",
    "    path = f\"s3a://{S3_BUCKET}/silver/funding_rate/*/*\"\n",
    "    try:\n",
    "        df = spark.read.parquet(path)\n",
    "    except:\n",
    "        print(\"   ‚ö†Ô∏è No Funding Rate data found\")\n",
    "        return None\n",
    "\n",
    "    if last_time:\n",
    "        # Buffer l·ªõn h∆°n cho funding v√¨ n√≥ th∆∞a (8h/l·∫ßn)\n",
    "        buffer_funding = last_time - timedelta(days=1)\n",
    "        # funding_time ƒë√£ l√† Timestamp, so s√°nh tr·ª±c ti·∫øp ƒë∆∞·ª£c\n",
    "        df = df.filter(F.col(\"funding_time\") >= F.lit(buffer_funding))\n",
    "\n",
    "    df = df.withColumnRenamed(\"funding_time\", \"timestamp\")\n",
    "\n",
    "    # ƒê·∫£m b·∫£o ki·ªÉu d·ªØ li·ªáu l√† TimestampType (cho ch·∫Øc ch·∫Øn)\n",
    "    df = df.withColumn(\"timestamp\", F.col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "    # Ch·ªâ l·∫•y c·ªôt c·∫ßn thi·∫øt\n",
    "    return df.select(\"timestamp\", F.col(\"funding_rate\").alias(\"funding_rate\"))\n",
    "\n",
    "def merge_features_for_interval(spark, interval):\n",
    "    print(f\"\\nüöÄ Processing Interval: {interval}\")\n",
    "    last_time = get_last_processed_time(interval)\n",
    "    data_frames = []\n",
    "\n",
    "    # 1. Load Sources Agg (Loop)\n",
    "    for alias, table_name in SOURCE_MAPPING.items():\n",
    "        print(f\"   üìÇ Loading {table_name}...\")\n",
    "        df = load_agg_source(spark, alias, table_name, interval, last_time)\n",
    "        if df is not None:\n",
    "            data_frames.append(df)\n",
    "\n",
    "    # 2. Load Funding (Load 1 l·∫ßn duy nh·∫•t b√™n ngo√†i v√≤ng l·∫∑p)\n",
    "    #funding_df = load_funding_rate(spark, last_time)\n",
    "\n",
    "    if not data_frames:\n",
    "        print(f\"   ‚ö†Ô∏è No agg data found for {interval}. Skipping.\")\n",
    "        return False\n",
    "\n",
    "    # 3. Merge Agg Data tr∆∞·ªõc\n",
    "    print(f\"   üîó Merging {len(data_frames)} agg sources...\")\n",
    "\n",
    "    # H√†m merge t·ªëi ∆∞u (b·ªè .count())\n",
    "    def join_dfs(df1, df2):\n",
    "        return df1.join(df2, on=\"timestamp\", how=\"full_outer\") \\\n",
    "                  .withColumn(\"timestamp\", F.coalesce(df1[\"timestamp\"], df2[\"timestamp\"]))\n",
    "\n",
    "    merged_df = reduce(join_dfs, data_frames)\n",
    "\n",
    "    # 4. Join & Forward Fill Funding Rate\n",
    "    # if funding_df is not None:\n",
    "    #     print(\"   üîó Merging Funding Rate (with Forward Fill)...\")\n",
    "    #     # Full Outer Join v·ªõi Funding\n",
    "    #     merged_df = merged_df.join(funding_df, on=\"timestamp\", how=\"full_outer\") \\\n",
    "    #                          .withColumn(\"timestamp\", F.coalesce(merged_df[\"timestamp\"], funding_df[\"timestamp\"]))\n",
    "    #\n",
    "    #     # S·∫Øp x·∫øp ƒë·ªÉ window function ch·∫°y ƒë√∫ng\n",
    "    #     merged_df = merged_df.orderBy(\"timestamp\")\n",
    "    #\n",
    "    #     # K·ªπ thu·∫≠t Forward Fill trong Spark\n",
    "    #     window_ffill = Window.orderBy(\"timestamp\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    #     merged_df = merged_df.withColumn(\n",
    "    #         \"funding_rate\",\n",
    "    #         F.last(\"funding_rate\", ignorenulls=True).over(window_ffill)\n",
    "    #     )\n",
    "\n",
    "    # 5. Clean & Enrich\n",
    "    merged_df = merged_df \\\n",
    "        .withColumn(\"interval\", F.lit(interval)) \\\n",
    "        .withColumn(\"symbol\", F.lit(SYMBOL)) \\\n",
    "        .withColumn(\"date_part\", F.date_format(\"timestamp\", \"yyyy-MM-dd\")) \\\n",
    "        .withColumn(\"processed_at\", F.current_timestamp())\n",
    "\n",
    "    # 6. Write Staging\n",
    "    output_path = f\"{PATH_GOLD_STAGING}/{interval}\"\n",
    "    print(f\"   üíæ Writing to Staging: {output_path}\")\n",
    "    merged_df.write.mode(\"overwrite\").partitionBy(\"date_part\").parquet(output_path)\n",
    "\n",
    "    # 7. Sync DuckDB\n",
    "    sync_to_duckdb(output_path, interval)\n",
    "    return True\n",
    "def process_and_merge_all(spark):\n",
    "    print(\"\\nüöÄ Starting Unified Merger Process...\")\n",
    "\n",
    "    all_interval_dfs = []\n",
    "\n",
    "    # 1. Loop qua c√°c interval v√† gom DF v√†o list\n",
    "    for interval in INTERVALS:\n",
    "        df = merge_features_for_interval(spark, interval)\n",
    "        if df is not None:\n",
    "            all_interval_dfs.append(df)\n",
    "\n",
    "    if not all_interval_dfs:\n",
    "        print(\"‚ùå No data found for any interval.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"\\nüîó Unioning {len(all_interval_dfs)} intervals into one DataFrame...\")\n",
    "\n",
    "    # 2. Union All (G·ªôp t·∫•t c·∫£ interval l·∫°i)\n",
    "    # unionByName: An to√†n h∆°n union th∆∞·ªùng n·∫øu th·ª© t·ª± c·ªôt b·ªã l·ªách\n",
    "    final_big_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), all_interval_dfs)\n",
    "\n",
    "    # 3. Write to Gold (Partition by date_part AND interval)\n",
    "    # Partition theo c·∫£ interval gi√∫p query nhanh h∆°n: WHERE interval='5m'\n",
    "    print(f\"üíæ Writing unified dataset to: {PATH_GOLD_STAGING}\")\n",
    "\n",
    "    final_big_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .partitionBy(\"date_part\", \"interval\") \\\n",
    "        .parquet(PATH_GOLD_STAGING)\n",
    "\n",
    "    # 4. Sync DuckDB (1 l·∫ßn duy nh·∫•t)\n",
    "    sync_to_duckdb(PATH_GOLD_STAGING)\n",
    "    return True\n",
    "def sync_to_duckdb(staging_path, interval):\n",
    "    \"\"\"Sync d·ªØ li·ªáu ƒë√£ merge v√†o DuckDB Fact Table\"\"\"\n",
    "    print(\"   ü¶Ü Syncing to DuckDB...\")\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "\n",
    "    # Path ƒë·ªçc cho DuckDB (s3:// thay v√¨ s3a://)\n",
    "    duck_read_path = staging_path.replace(\"s3a://\", \"s3://\") + \"/*/*/*.parquet\"\n",
    "\n",
    "    try:\n",
    "        with open(DUCKDB_INIT_SCRIPT, 'r') as f:\n",
    "            con.execute(f.read())\n",
    "\n",
    "        # 1. T·∫°o b·∫£ng Fact (Schema Evolution)\n",
    "        # S·ª≠ d·ª•ng m·∫πo: T·∫°o table t·ª´ file parquet r·ªóng ƒë·ªÉ l·∫•y schema t·ª± ƒë·ªông\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS fact_merged_features AS\n",
    "            SELECT * FROM read_parquet('{duck_read_path}', hive_partitioning=1) LIMIT 0\n",
    "        \"\"\")\n",
    "\n",
    "        # 2. X√≥a d·ªØ li·ªáu c≈© tr√πng l·∫∑p (Idempotency)\n",
    "        con.execute(f\"\"\"\n",
    "            DELETE FROM fact_merged_features\n",
    "            WHERE interval = '{interval}'\n",
    "            AND timestamp IN (\n",
    "                SELECT timestamp FROM read_parquet('{duck_read_path}', hive_partitioning=1)\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # 3. Insert d·ªØ li·ªáu m·ªõi\n",
    "        # S·ª≠ d·ª•ng BY NAME ƒë·ªÉ map c·ªôt t·ª± ƒë·ªông b·∫•t k·ªÉ th·ª© t·ª±\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO fact_merged_features BY NAME\n",
    "            SELECT * FROM read_parquet('{duck_read_path}', hive_partitioning=1)\n",
    "        \"\"\")\n",
    "        print(\"   ‚úÖ Sync Complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"No files found\" in str(e):\n",
    "            print(\"   ‚ö†Ô∏è No new files to sync.\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå DuckDB Error: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "# ==========================================\n",
    "# 4. MAIN ENTRYPOINT\n",
    "# ==========================================\n",
    "def main():\n",
    "    spark = get_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    print(\"===========================================\")\n",
    "    print(\"   GOLD LAYER: FEATURE MERGER PIPELINE    \")\n",
    "    print(\"===========================================\")\n",
    "\n",
    "    # for interval in INTERVALS:\n",
    "    #     try:\n",
    "    #         merge_features_for_interval(spark, interval)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"‚ùå Error processing {interval}: {e}\")\n",
    "    process_and_merge_all(spark)\n",
    "    spark.stop()\n",
    "    print(\"\\nüèÅ Pipeline Finished.\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "6f3185ef3727217e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "create features",
   "id": "8ec6027ff6d6f1be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import duckdb\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DoubleType, TimestampType\n",
    "\n",
    "# --- CONFIG ---\n",
    "S3_ENDPOINT = \"http://localhost:9000\"\n",
    "S3_ACCESS = \"minio\"\n",
    "S3_SECRET = \"minio123\"\n",
    "S3_BUCKET = \"trading-okx\"\n",
    "DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'\n",
    "STAGING_OUTPUT_PATH = f\"s3a://{S3_BUCKET}/gold/staging_derive\"\n",
    "FINAL_OUTPUT_PATH = f\"s3a://{S3_BUCKET}/gold/derive_final\"\n",
    "INPUT_PATH = f\"s3a://{S3_BUCKET}/gold/staging_merged_features/\"\n",
    "INTERVALS = [\"5m\", \"15m\", \"1h\", \"4h\", \"1d\"]\n",
    "def get_spark_session():\n",
    "    return SparkSession.builder \\\n",
    "        .appName(\"Derived_Features_Creator\") \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET) \\\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "        .getOrCreate()\n",
    "def get_last_processed_time():\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    try:\n",
    "        with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "            sql_script = f.read()\n",
    "        con.execute(sql_script)\n",
    "\n",
    "        table_exists = con.execute(\n",
    "            \"SELECT count(*) FROM information_schema.tables WHERE table_name = 'fact_derived_features'\"\n",
    "        ).fetchone()[0]\n",
    "\n",
    "        if table_exists == 0:\n",
    "            return None\n",
    "\n",
    "        result = con.execute(\n",
    "            \"SELECT MAX(timestamp) FROM fact_derived_features\"\n",
    "        ).fetchone()[0]\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not get last processed time: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        con.close()\n",
    "def load_data(spark):\n",
    "    \"\"\"Load merged data t·ª´ MinIO v·ªõi incremental n·∫øu c√≥\"\"\"\n",
    "    last_time = get_last_processed_time()\n",
    "    df = spark.read.parquet(INPUT_PATH)\n",
    "\n",
    "    if last_time:\n",
    "        start_timestamp = last_time - timedelta(days=1)  # Buffer 1 ng√†y\n",
    "        print(f\" ‚ÑπÔ∏è Incremental load: timestamp >= {start_timestamp}\")\n",
    "        df = df.filter(F.col(\"timestamp\") >= F.lit(start_timestamp))\n",
    "\n",
    "    if df.rdd.isEmpty():\n",
    "        print(\"Error: No data loaded\")\n",
    "        return None\n",
    "\n",
    "    # Chu·∫©n h√≥a timestamp\n",
    "    df = df.withColumn(\"timestamp\", F.col(\"timestamp\").cast(TimestampType()))\n",
    "\n",
    "    row_count = df.count()\n",
    "    print(f\"‚úì Loaded input data: {row_count} rows\")\n",
    "    return df\n",
    "def create_macro_basis_features(df):\n",
    "    \"\"\"T·∫°o Macro & Basis features - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating Macro & Basis features...\")\n",
    "    for interval in INTERVALS:\n",
    "        prefix_trade = f\"trades_{interval}_\"\n",
    "        prefix_index = f\"index_price_{interval}_\"\n",
    "        prefix_mark = f\"mark_price_{interval}_\"\n",
    "\n",
    "        price_mean_trade = f\"{prefix_trade}price_mean_trade\"\n",
    "        index_mean = f\"{prefix_index}mean\"\n",
    "        mark_mean = f\"{prefix_mark}mean\"\n",
    "        price_std_trade = f\"{prefix_trade}price_std_trade\"\n",
    "        index_std = f\"{prefix_index}std\"\n",
    "\n",
    "        if all(c in df.columns for c in [price_mean_trade, index_mean, mark_mean]):\n",
    "            df = df.withColumn(f\"feat_basis_spread_{interval}\", F.col(price_mean_trade) - F.col(index_mean))\n",
    "            df = df.withColumn(f\"feat_basis_ratio_{interval}\", (F.col(price_mean_trade) - F.col(index_mean)) / (F.col(index_mean) + 1e-9))\n",
    "            df = df.withColumn(f\"feat_premium_index_{interval}\", (F.col(mark_mean) - F.col(index_mean)) / (F.col(index_mean) + 1e-9))\n",
    "            if price_std_trade in df.columns and index_std in df.columns:\n",
    "                df = df.withColumn(f\"feat_volatility_spread_{interval}\", F.col(price_std_trade) - F.col(index_std))\n",
    "    return df\n",
    "def create_log_returns(df):\n",
    "    \"\"\"T·∫°o log returns v√† trend divergence - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating log returns...\")\n",
    "    window = Window.orderBy(\"timestamp\")\n",
    "\n",
    "    for interval in INTERVALS:\n",
    "        prefix_orderbook = f\"orderbook_{interval}_\"\n",
    "        prefix_index = f\"index_price_{interval}_\"\n",
    "\n",
    "        wmp_last = f\"{prefix_orderbook}wmp_last\"\n",
    "        index_close = f\"{prefix_index}close\"\n",
    "\n",
    "        if wmp_last in df.columns:\n",
    "            df = df.withColumn(f\"feat_log_return_trade_{interval}\", F.log(F.col(wmp_last) / (F.lag(F.col(wmp_last), 1).over(window) + 1e-9)))\n",
    "\n",
    "        if index_close in df.columns:\n",
    "            df = df.withColumn(f\"feat_log_return_index_{interval}\", F.log(F.col(index_close) / (F.lag(F.col(index_close), 1).over(window) + 1e-9)))\n",
    "\n",
    "        # Trend divergence sau khi c√≥ log returns\n",
    "        trade_ret = f\"feat_log_return_trade_{interval}\"\n",
    "        index_ret = f\"feat_log_return_index_{interval}\"\n",
    "        if trade_ret in df.columns and index_ret in df.columns:\n",
    "            df = df.withColumn(f\"feat_trend_divergence_{interval}\", F.col(trade_ret) - F.col(index_ret))\n",
    "\n",
    "    return df\n",
    "def create_funding_sentiment_features(df):\n",
    "    \"\"\"T·∫°o funding sentiment features - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating Funding & Sentiment features...\")\n",
    "    window_12 = Window.orderBy(\"timestamp\").rowsBetween(-11, 0)  # 12 k·ª≥\n",
    "\n",
    "    for interval in INTERVALS:\n",
    "        prefix_orderbook = f\"orderbook_{interval}_\"\n",
    "\n",
    "        wmp_last = f\"{prefix_orderbook}wmp_last\"\n",
    "        funding_rate = \"funding_funding_rate\"\n",
    "        basis_ratio = f\"feat_basis_ratio_{interval}\"\n",
    "\n",
    "        if wmp_last in df.columns and funding_rate in df.columns:\n",
    "            df = df.withColumn(f\"feat_funding_cost_{interval}\", F.col(funding_rate) * F.col(wmp_last))\n",
    "            df = df.withColumn(f\"feat_funding_trend_{interval}\", F.col(funding_rate) - F.mean(funding_rate).over(window_12))\n",
    "\n",
    "        if basis_ratio in df.columns:\n",
    "            df = df.withColumn(f\"feat_funding_basis_corr_{interval}\", F.col(funding_rate) * F.col(basis_ratio))\n",
    "\n",
    "    return df\n",
    "def create_momentum_trend_features(df):\n",
    "    \"\"\"T·∫°o momentum features - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating Momentum & Trend features...\")\n",
    "    window_3 = Window.orderBy(\"timestamp\").rowsBetween(-2, 0)  # 3 k·ª≥\n",
    "    window_12 = Window.orderBy(\"timestamp\").rowsBetween(-11, 0)  # 12 k·ª≥\n",
    "\n",
    "    for interval in INTERVALS:\n",
    "        prefix_orderbook = f\"orderbook_{interval}_\"\n",
    "\n",
    "        wmp_last = f\"{prefix_orderbook}wmp_last\"\n",
    "        wmp_min = f\"{prefix_orderbook}wmp_min\"\n",
    "        wmp_max = f\"{prefix_orderbook}wmp_max\"\n",
    "\n",
    "        if all(c in df.columns for c in [wmp_last, wmp_min, wmp_max]):\n",
    "            df = df.withColumn(f\"feat_price_velocity_{interval}\", (F.col(wmp_last) - F.lag(F.col(wmp_last), 3)) / 3)\n",
    "            df = df.withColumn(f\"feat_ma_divergence_{interval}\", F.col(wmp_last) / (F.mean(wmp_last).over(window_12) + 1e-9) - 1)\n",
    "            df = df.withColumn(f\"feat_rsi_proxy_{interval}\", (F.col(wmp_last) - F.col(wmp_min)) / (F.col(wmp_max) - F.col(wmp_min) + 1e-9))\n",
    "\n",
    "            # Acceleration sau velocity\n",
    "            velocity_col = f\"feat_price_velocity_{interval}\"\n",
    "            df = df.withColumn(f\"feat_price_accel_{interval}\", F.col(velocity_col) - F.lag(F.col(velocity_col), 1))\n",
    "\n",
    "    return df\n",
    "def create_order_flow_features(df):\n",
    "    \"\"\"T·∫°o order flow features - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating Order Flow features...\")\n",
    "    for interval in INTERVALS:\n",
    "        prefix_trades = f\"trades_{interval}_\"\n",
    "        prefix_orderbook = f\"orderbook_{interval}_\"\n",
    "\n",
    "        volume_buy = f\"{prefix_trades}volume_buy\"\n",
    "        volume_sell = f\"{prefix_trades}volume_sell\"\n",
    "        volume_total = f\"{prefix_trades}volume_total\"\n",
    "        count_buy = f\"{prefix_trades}count_buy\"\n",
    "        count_sell = f\"{prefix_trades}count_sell\"\n",
    "\n",
    "        sum_bid_50 = f\"{prefix_orderbook}sum_bid_50\"\n",
    "        sum_ask_50 = f\"{prefix_orderbook}sum_ask_50\"\n",
    "        total_depth_50 = f\"{prefix_orderbook}total_depth_50\"\n",
    "\n",
    "        if all(c in df.columns for c in [volume_buy, volume_sell, volume_total]):\n",
    "            df = df.withColumn(f\"feat_trade_imbalance_{interval}\", (F.col(volume_buy) - F.col(volume_sell)) / (F.col(volume_total) + 1e-9))\n",
    "\n",
    "        if all(c in df.columns for c in [sum_bid_50, sum_ask_50, total_depth_50]):\n",
    "            df = df.withColumn(f\"feat_depth_imbalance_{interval}\", (F.col(sum_bid_50) - F.col(sum_ask_50)) / (F.col(total_depth_50) + 1e-9))\n",
    "            df = df.withColumn(f\"feat_buy_consumption_{interval}\", F.col(volume_buy) / (F.col(sum_ask_50) + 1e-9))\n",
    "            df = df.withColumn(f\"feat_sell_consumption_{interval}\", F.col(volume_sell) / (F.col(sum_bid_50) + 1e-9))\n",
    "\n",
    "        if all(c in df.columns for c in [count_buy, count_sell]):\n",
    "            df = df.withColumn(f\"feat_aggressiveness_{interval}\", F.col(count_buy) / (F.col(count_sell) + 1e-9))\n",
    "\n",
    "        # Smart money divergence sau imbalance\n",
    "        trade_imbal = f\"feat_trade_imbalance_{interval}\"\n",
    "        depth_imbal = f\"feat_depth_imbalance_{interval}\"\n",
    "        if trade_imbal in df.columns and depth_imbal in df.columns:\n",
    "            df = df.withColumn(f\"feat_smart_money_div_{interval}\", F.col(trade_imbal) - F.col(depth_imbal))\n",
    "\n",
    "    return df\n",
    "def create_volatility_liquidity_features(df):\n",
    "    \"\"\"T·∫°o volatility & liquidity - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating Volatility & Liquidity features...\")\n",
    "    for interval in INTERVALS:\n",
    "        prefix_orderbook = f\"orderbook_{interval}_\"\n",
    "        prefix_trades = f\"trades_{interval}_\"\n",
    "\n",
    "        spread_mean = f\"{prefix_orderbook}spread_mean\"\n",
    "        wmp_mean = f\"{prefix_orderbook}wmp_mean\"\n",
    "        total_depth_50 = f\"{prefix_orderbook}total_depth_50\"\n",
    "\n",
    "        price_max_trade = f\"{prefix_trades}price_max_trade\"\n",
    "        price_min_trade = f\"{prefix_trades}price_min_trade\"\n",
    "        price_mean_trade = f\"{prefix_trades}price_mean_trade\"\n",
    "        price_last_trade = f\"{prefix_trades}price_last_trade\"\n",
    "\n",
    "        if spread_mean in df.columns and wmp_mean in df.columns:\n",
    "            df = df.withColumn(f\"feat_rel_spread_{interval}\", F.col(spread_mean) / (F.col(wmp_mean) + 1e-9))\n",
    "\n",
    "        if total_depth_50 in df.columns and spread_mean in df.columns:\n",
    "            df = df.withColumn(f\"feat_liq_density_{interval}\", F.col(total_depth_50) / (F.col(spread_mean) + 1e-9))\n",
    "\n",
    "        if all(c in df.columns for c in [price_max_trade, price_min_trade, price_mean_trade, price_last_trade]):\n",
    "            df = df.withColumn(f\"feat_candle_range_{interval}\", (F.col(price_max_trade) - F.col(price_min_trade)) / (F.col(price_mean_trade) + 1e-9))\n",
    "            df = df.withColumn(f\"feat_tail_extension_{interval}\", (F.col(price_max_trade) - F.col(price_last_trade)) / (F.col(price_max_trade) - F.col(price_min_trade) + 1e-9))\n",
    "\n",
    "    return df\n",
    "def create_time_features(df):\n",
    "    \"\"\"T·∫°o time features - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating Time features...\")\n",
    "    df = df.withColumn(\"hour\", F.hour(\"timestamp\"))\n",
    "    df = df.withColumn(\"feat_hour_sin\", F.sin(2 * 3.141592653589793 * F.col(\"hour\") / 24))\n",
    "    df = df.withColumn(\"feat_hour_cos\", F.cos(2 * 3.141592653589793 * F.col(\"hour\") / 24))\n",
    "    return df.drop(\"hour\")\n",
    "def create_efficiency_features(df):\n",
    "    \"\"\"T·∫°o efficiency features - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating Market Efficiency features...\")\n",
    "    window_12 = Window.orderBy(\"timestamp\").rowsBetween(-11, 0)\n",
    "\n",
    "    for interval in INTERVALS:\n",
    "        prefix_orderbook = f\"orderbook_{interval}_\"\n",
    "        wmp_last = f\"{prefix_orderbook}wmp_last\"\n",
    "\n",
    "        if wmp_last in df.columns:\n",
    "            df = df.withColumn(\"price_diff\", F.abs(F.col(wmp_last) - F.lag(F.col(wmp_last), 1)))\n",
    "            df = df.withColumn(f\"feat_efficiency_ratio_{interval}\", F.abs(F.col(wmp_last) - F.lag(F.col(wmp_last), 12)) / (F.sum(\"price_diff\").over(window_12) + 1e-9))\n",
    "            df = df.withColumn(f\"feat_price_entropy_{interval}\", F.stddev(\"wmp_last\").over(window_12) / (F.mean(\"wmp_last\").over(window_12) + 1e-9))\n",
    "    return df.drop(\"price_diff\") if \"price_diff\" in df.columns else df\n",
    "def create_orderbook_shape_features(df):\n",
    "    \"\"\"T·∫°o orderbook shape - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating Orderbook Shape features...\")\n",
    "    for interval in INTERVALS:\n",
    "        prefix = f\"orderbook_{interval}_\"\n",
    "\n",
    "        sum_bid_20 = f\"{prefix}sum_bid_20\"\n",
    "        sum_ask_20 = f\"{prefix}sum_ask_20\"\n",
    "        sum_bid_5 = f\"{prefix}sum_bid_5\"\n",
    "        sum_bid_50 = f\"{prefix}sum_bid_50\"\n",
    "        best_bid = f\"{prefix}best_bid\"\n",
    "        best_ask = f\"{prefix}best_ask\"\n",
    "        bid_px_20 = f\"{prefix}bid_px_20\"\n",
    "        ask_px_20 = f\"{prefix}ask_px_20\"\n",
    "\n",
    "        if all(c in df.columns for c in [sum_bid_20, sum_ask_20, best_bid, best_ask, bid_px_20, ask_px_20]):\n",
    "            df = df.withColumn(f\"feat_bid_slope_{interval}\", F.col(sum_bid_20) / (F.col(best_bid) - F.col(bid_px_20) + 1e-9))\n",
    "            df = df.withColumn(f\"feat_ask_slope_{interval}\", F.col(sum_ask_20) / (F.col(ask_px_20) - F.col(best_ask) + 1e-9))\n",
    "            bid_slope = f\"feat_bid_slope_{interval}\"\n",
    "            ask_slope = f\"feat_ask_slope_{interval}\"\n",
    "            df = df.withColumn(f\"feat_slope_imbalance_{interval}\", F.col(bid_slope) / (F.col(ask_slope) + 1e-9))\n",
    "\n",
    "        if all(c in df.columns for c in [sum_bid_5, sum_bid_50]):\n",
    "            df = df.withColumn(f\"feat_depth_convexity_{interval}\", F.col(sum_bid_5) / (F.col(sum_bid_50) + 1e-9))\n",
    "\n",
    "    return df\n",
    "def create_statistical_normalization(df):\n",
    "    \"\"\"T·∫°o Z-scores - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating Statistical Normalization features...\")\n",
    "    window_1h = Window.orderBy(\"timestamp\").rowsBetween(-11, 0)  # 12 k·ª≥ = 1h cho 5m base\n",
    "\n",
    "    for interval in INTERVALS:\n",
    "        prefix_trades = f\"trades_{interval}_\"\n",
    "        prefix_orderbook = f\"orderbook_{interval}_\"\n",
    "\n",
    "        volume_total = f\"{prefix_trades}volume_total\"\n",
    "        spread_mean = f\"{prefix_orderbook}spread_mean\"\n",
    "        trade_imbalance = f\"feat_trade_imbalance_{interval}\"\n",
    "\n",
    "        if volume_total in df.columns:\n",
    "            df = df.withColumn(f\"feat_z_volume_{interval}\", (F.col(volume_total) - F.mean(volume_total).over(window_1h)) / (F.stddev(volume_total).over(window_1h) + 1e-9))\n",
    "\n",
    "        if spread_mean in df.columns:\n",
    "            df = df.withColumn(f\"feat_z_spread_{interval}\", (F.col(spread_mean) - F.mean(spread_mean).over(window_1h)) / (F.stddev(spread_mean).over(window_1h) + 1e-9))\n",
    "\n",
    "        if trade_imbalance in df.columns:\n",
    "            df = df.withColumn(f\"feat_z_imbalance_{interval}\", (F.col(trade_imbalance) - F.mean(trade_imbalance).over(window_1h)) / (F.stddev(trade_imbalance).over(window_1h) + 1e-9))\n",
    "\n",
    "    return df\n",
    "def create_vwap_pivot_features(df):\n",
    "    \"\"\"T·∫°o VWAP & Pivot features - gi·ªëng Polars\"\"\"\n",
    "    print(\"üîß Creating VWAP & Pivot features...\")\n",
    "    window_1h = Window.orderBy(\"timestamp\").rowsBetween(-11, 0)\n",
    "\n",
    "    for interval in INTERVALS:\n",
    "        prefix_orderbook = f\"orderbook_{interval}_\"\n",
    "        prefix_trades = f\"trades_{interval}_\"\n",
    "\n",
    "        wmp_last = f\"{prefix_orderbook}wmp_last\"\n",
    "        vwap = f\"{prefix_trades}vwap\"  # Gi·∫£ ƒë·ªãnh c√≥ vwap t·ª´ trades\n",
    "\n",
    "        if wmp_last in df.columns and vwap in df.columns:\n",
    "            df = df.withColumn(f\"feat_dist_vwap_{interval}\", (F.col(wmp_last) - F.col(vwap)) / (F.col(vwap) + 1e-9))\n",
    "            df = df.withColumn(f\"feat_dist_max_{interval}\", (F.col(wmp_last) - F.max(wmp_last).over(window_1h)) / (F.max(wmp_last).over(window_1h) + 1e-9))\n",
    "            df = df.withColumn(f\"feat_dist_min_{interval}\", (F.col(wmp_last) - F.min(wmp_last).over(window_1h)) / (F.min(wmp_last).over(window_1h) + 1e-9))\n",
    "\n",
    "    return df\n",
    "def create_all_features(df):\n",
    "    \"\"\"G·ªçi tu·∫ßn t·ª± t·∫•t c·∫£ feature creator - gi·ªëng Polars\"\"\"\n",
    "    print(\"üöÄ Creating derived features...\")\n",
    "    df = df.orderBy(\"timestamp\")\n",
    "\n",
    "    df = create_log_returns(df)\n",
    "    df = create_macro_basis_features(df)\n",
    "    df = create_funding_sentiment_features(df)\n",
    "    df = create_momentum_trend_features(df)\n",
    "    df = create_order_flow_features(df)\n",
    "    df = create_volatility_liquidity_features(df)\n",
    "    df = create_time_features(df)\n",
    "    df = create_efficiency_features(df)\n",
    "    df = create_orderbook_shape_features(df)\n",
    "    df = create_statistical_normalization(df)\n",
    "    df = create_vwap_pivot_features(df)\n",
    "\n",
    "    return df\n",
    "def save_features_data(df):\n",
    "    \"\"\"L∆∞u staging + upsert DuckDB + l∆∞u final gold - gi·ªëng merge code\"\"\"\n",
    "    if df is None:\n",
    "        return\n",
    "\n",
    "    # Th√™m date_part ƒë·ªÉ partition\n",
    "    #df = df.withColumn(\"date_part\", F.date_format(\"timestamp\", \"yyyy-MM-dd\"))\n",
    "\n",
    "    # L∆∞u staging\n",
    "    print(f\" üíæ Writing staging to {STAGING_OUTPUT_PATH}\")\n",
    "    df.write.mode(\"overwrite\").partitionBy(\"date_part\").parquet(STAGING_OUTPUT_PATH)\n",
    "\n",
    "    # Upsert v√†o DuckDB\n",
    "    con = duckdb.connect(DUCKDB_PATH)\n",
    "    try:\n",
    "        with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:\n",
    "            sql_script = f.read()\n",
    "        con.execute(sql_script)\n",
    "\n",
    "        # T·∫°o b·∫£ng fact n·∫øu ch∆∞a c√≥\n",
    "        con.execute(f\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS fact_derived_features AS\n",
    "            SELECT *  EXCLUDE date_part FROM read_parquet('{STAGING_OUTPUT_PATH}/*/*.parquet', hive_partitioning=1) LIMIT 0\n",
    "        \"\"\")\n",
    "\n",
    "        # X√≥a d·ªØ li·ªáu c≈© tr√πng timestamp\n",
    "        print(\" üîÑ Cleaning overlapping data...\")\n",
    "        con.execute(f\"\"\"\n",
    "            DELETE FROM fact_derived_features\n",
    "            WHERE timestamp IN (\n",
    "                SELECT timestamp FROM read_parquet('{STAGING_OUTPUT_PATH}/*/*.parquet', hive_partitioning=1)\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Insert d·ªØ li·ªáu m·ªõi\n",
    "        print(\" üì• Inserting new data...\")\n",
    "        con.execute(f\"\"\"\n",
    "            INSERT INTO fact_derived_features\n",
    "            SELECT * EXCLUDE date_part\n",
    "            FROM read_parquet('{STAGING_OUTPUT_PATH}/*/*.parquet', hive_partitioning=1)\n",
    "        \"\"\")\n",
    "        print(\" ‚úÖ DuckDB upsert completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\" ‚ö†Ô∏è DuckDB Error: {e}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "    # L∆∞u final gold (kh√¥ng partition)\n",
    "    print(f\" üíæ Final gold saved to {FINAL_OUTPUT_PATH}\")\n",
    "    df.drop(\"date_part\").write.mode(\"overwrite\").parquet(FINAL_OUTPUT_PATH)\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = get_spark_session()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "    print(\"STARTING DERIVED FEATURES PIPELINE...\")\n",
    "\n",
    "    try:\n",
    "        df = load_data(spark)\n",
    "        if df is None:\n",
    "            print(\"No data loaded. Exiting.\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        enhanced_df = create_all_features(df)\n",
    "        save_features_data(enhanced_df)\n",
    "        print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed: {e}\")\n",
    "        sys.exit(1)\n",
    "    finally:\n",
    "        spark.stop()\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "a87c6e970cb1931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "from dotenv import load_dotenv",
   "id": "51444525879a52d4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
