import duckdb
from datetime import timedelta
from pyspark.sql import functions as F
from pyspark.sql.types import TimestampType
from functools import reduce
from plugins.spark_config import S3_BUCKET, DUCKDB_PATH

PATH_SILVER_BASE = f"s3a://trading-okx/silver"
#SYMBOL= "btc-usdt-swap"
SYMBOL= "btc-usdt"
PATH_GOLD_STAGING = f"s3a://{S3_BUCKET}/gold/staging_merged_features"
PATH_GOLD_FINAL = f"s3a://{S3_BUCKET}/gold/fact_merged_features"
SOURCE_MAPPING = {
    "trade": "agg_trades",
    "book": "agg_orderbook",
    "index": "indexPriceKlines",
    "mark": "markPriceKlines"
}
INTERVALS = ["1m","5m", "15m", "1h", "4h", "1d"]
# --- 1. UTILS ---
def get_last_processed_time(interval):
    """Láº¥y timestamp cuá»‘i cÃ¹ng tá»« DuckDB Ä‘á»ƒ cháº¡y Incremental"""
    con = duckdb.connect(DUCKDB_PATH)
    try:
        # Load MinIO config
        with open('/opt/airflow/SQL_db/config_dw/warehouse_source.sql', 'r') as f:
            con.execute(f.read())

        # Check table existence
        exists = con.execute(
            "SELECT count(*) FROM information_schema.tables WHERE table_name = 'fact_merged_features'"
        ).fetchone()[0]

        if exists == 0: return None

        # Láº¥y max time cá»§a interval tÆ°Æ¡ng á»©ng
        query = f"SELECT MAX(timestamp) FROM fact_merged_features WHERE interval = '{interval}'"
        result = con.execute(query).fetchone()[0]
        return result
    except Exception as e:
        print(f"âš ï¸ DuckDB Info: {e}")
        return None
    finally:
        con.close()
def load_agg_source(spark, source_alias, table_name, interval, last_time):
    path = f"{PATH_SILVER_BASE}/{table_name}/{interval}/"
    try:
        df = spark.read.parquet(path + "*")
    except Exception:
        # print(f"   âš ï¸ Path not found: {path}") # Bá»›t log rÃ¡c
        return None

    if last_time:
        buffer_time = last_time - timedelta(hours=1)
        start_date_str = buffer_time.strftime("%Y-%m-%d")

        # Pruning Partition
        if "date_part" in df.columns:
            df = df.filter(F.col("date_part") >= F.lit(start_date_str))

        # Filter Time
        time_col = "candle_time" if "candle_time" in df.columns else "close_time"
        if time_col in df.columns:
            df = df.filter(F.col(time_col) >= F.lit(buffer_time))

    if df.rdd.isEmpty(): return None

    # Standardization
    for t_col in ["close_time", "window_end", "candle_time"]:
        if t_col in df.columns:
            df = df.withColumnRenamed(t_col, "timestamp")
            break

    df = df.withColumn("timestamp", F.col("timestamp").cast(TimestampType()))

    # Renaming & Cleanup
    exclude_cols = ["timestamp", "symbol", "interval", "date_part", "ingestion_time"]
    rename_mapping = {}
    for col_name in df.columns:
        if col_name not in exclude_cols:
            rename_mapping[col_name] = f"{source_alias}_{col_name}"

    for old, new in rename_mapping.items():
        df = df.withColumnRenamed(old, new)

    # Drop metadata columns cá»§a Silver Ä‘á»ƒ trÃ¡nh trÃ¹ng khi Union/Join
    for col in ["symbol", "interval", "date_part", "ingestion_time"]:
        if col in df.columns: df = df.drop(col)

    return df
def load_funding_rate(spark, last_time):
    # ÄÆ°á»ng dáº«n funding rate
    path = f"s3a://{S3_BUCKET}/silver/funding_rate/*/*"
    try:
        df = spark.read.parquet(path)
    except:
        print("   âš ï¸ No Funding Rate data found")
        return None

    if last_time:
        # Buffer lá»›n hÆ¡n cho funding vÃ¬ nÃ³ thÆ°a (8h/láº§n)
        buffer_funding = last_time - timedelta(days=1)
        # funding_time Ä‘Ã£ lÃ  Timestamp, so sÃ¡nh trá»±c tiáº¿p Ä‘Æ°á»£c
        df = df.filter(F.col("funding_time") >= F.lit(buffer_funding))

    df = df.withColumnRenamed("funding_time", "timestamp")

    # Äáº£m báº£o kiá»ƒu dá»¯ liá»‡u lÃ  TimestampType (cho cháº¯c cháº¯n)
    df = df.withColumn("timestamp", F.col("timestamp").cast(TimestampType()))

    # Chá»‰ láº¥y cá»™t cáº§n thiáº¿t
    return df.select("timestamp", F.col("funding_rate").alias("funding_rate"))
def merge_features_for_interval(spark, interval):
    print(f"\nğŸš€ Processing Interval: {interval}")
    last_time = get_last_processed_time(interval)
    data_frames = []

    # 1. Load Sources Agg (Loop)
    for alias, table_name in SOURCE_MAPPING.items():
        print(f"   ğŸ“‚ Loading {table_name}...")
        df = load_agg_source(spark, alias, table_name, interval, last_time)
        if df is not None:
            data_frames.append(df)

    if not data_frames:
        print(f"   âš ï¸ No agg data found for {interval}. Skipping.")
        # --- Sá»¬A Táº I ÄÃ‚Y ---
        # CÅ©: return False  <-- NguyÃªn nhÃ¢n gÃ¢y lá»—i
        # Má»›i: return None
        return None

    # 3. Merge Agg Data trÆ°á»›c
    print(f"   ğŸ”— Merging {len(data_frames)} agg sources...")

    # HÃ m merge tá»‘i Æ°u (bá» .count())
    def join_dfs(df1, df2):
        return df1.join(df2, on="timestamp", how="full_outer") \
                  .withColumn("timestamp", F.coalesce(df1["timestamp"], df2["timestamp"]))

    merged_df = reduce(join_dfs, data_frames)

    # 5. Clean & Enrich
    merged_df = merged_df \
        .withColumn("interval", F.lit(interval)) \
        .withColumn("symbol", F.lit(SYMBOL)) \
        .withColumn("date_part", F.date_format("timestamp", "yyyy-MM-dd")) \
        .withColumn("processed_at", F.current_timestamp())

    return merged_df
def sync_unified_to_duckdb(staging_path):
    print("   ğŸ¦† Syncing Unified Data to DuckDB...")
    con = duckdb.connect(DUCKDB_PATH)

    # DuckDB Ä‘á»c S3 dÃ¹ng s3://
    duck_read_path = staging_path.replace("s3a://", "s3://") + "/*/*.parquet"

    try:
        with open('/opt/airflow/SQL_db/config_dw/warehouse_source.sql', 'r') as f:
            con.execute(f.read())

        # 1. Táº¡o báº£ng (Schema Evolution)
        con.execute(f"""
            CREATE TABLE IF NOT EXISTS fact_merged_features AS
            SELECT * FROM read_parquet('{duck_read_path}', hive_partitioning=1) LIMIT 0
        """)

        # 2. XÃ³a dá»¯ liá»‡u cÅ© (Dá»±a trÃªn cáº·p interval + timestamp cÃ³ trong file má»›i)
        print("   ğŸ”„ Cleaning overlaps...")
        con.execute(f"""
            DELETE FROM fact_merged_features
            WHERE (interval, timestamp) IN (
                SELECT interval, timestamp 
                FROM read_parquet('{duck_read_path}', hive_partitioning=1)
            )
        """)

        # 3. Insert dá»¯ liá»‡u má»›i
        print("   ğŸ“¥ Inserting new batch...")
        con.execute(f"""
            INSERT INTO fact_merged_features BY NAME
            SELECT * FROM read_parquet('{duck_read_path}', hive_partitioning=1)
        """)
        print("   âœ… Sync Complete.")

    except Exception as e:
        if "No files found" in str(e):
            print("   âš ï¸ No new files to sync.")
        else:
            print(f"   âŒ DuckDB Error: {e}")
    finally:
        con.close()
def process_and_merge_all(spark):

    print("\nğŸš€ Starting Unified Merger Process...")
    all_interval_dfs = []

    for interval in INTERVALS:
        # HÃ m nÃ y giá» Ä‘Ã£ tráº£ vá» DataFrame (hoáº·c None)
        df = merge_features_for_interval(spark, interval)
        if df is not None:
            all_interval_dfs.append(df)

    if not all_interval_dfs:
        print("âŒ No data found for any interval.")
        return False

    print(f"\nğŸ”— Unioning {len(all_interval_dfs)} intervals into one DataFrame...")

    # Union táº¥t cáº£ láº¡i
    final_big_df = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), all_interval_dfs)

    print(f"ğŸ’¾ Writing unified dataset to: {PATH_GOLD_STAGING}")

    # Ghi 1 láº§n duy nháº¥t, Partition theo cáº£ date_part vÃ  interval
    final_big_df.write \
        .mode("overwrite") \
        .partitionBy("date_part") \
        .parquet(PATH_GOLD_STAGING)

    # Gá»i hÃ m Sync má»›i
    sync_unified_to_duckdb(PATH_GOLD_STAGING)
    return True
def merge_features(spark):
    spark.sparkContext.setLogLevel("ERROR")
    print("===========================================")
    print("   GOLD LAYER: FEATURE MERGER PIPELINE    ")
    print("===========================================")
    process_and_merge_all(spark)
    print("\nğŸ Pipeline Finished.")
#merge_features()