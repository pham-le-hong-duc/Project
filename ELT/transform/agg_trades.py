import duckdb
from datetime import datetime, timedelta
from pyspark.sql import functions as F
from plugins.spark_config import S3_BUCKET, DUCKDB_PATH

def parse_interval_to_minutes(interval_str):
    unit = interval_str[-1].lower()
    try:
        value = int(interval_str[:-1])
    except:
        return 60 # Default fallback

    if unit == 'm': return value
    if unit == 'h': return value * 60
    if unit == 'd': return value * 1440
    return 60
def get_last_processed_time(interval_name):
    con = duckdb.connect(DUCKDB_PATH)
    try:
        # Ki·ªÉm tra b·∫£ng c√≥ t·ªìn t·∫°i kh√¥ng
        table_exists = con.execute("SELECT count(*) FROM information_schema.tables WHERE table_name = 'fact_market_features'").fetchone()[0]
        if table_exists == 0:
            return None

        query = f"SELECT MAX(candle_time) FROM fact_market_features WHERE interval = '{interval_name}'"
        result = con.execute(query).fetchone()[0]
        return result # Tr·∫£ v·ªÅ datetime ho·∫∑c None
    except:
        return None
    finally:
        con.close()
def calculate_features_incremental(spark, interval_name, interval_window):
    print(f"\nüöÄ Processing Features: {interval_name} ({interval_window})")
    # 1. T√çNH TO√ÅN BUFFER DYNAMIC (Quan tr·ªçng)
    interval_minutes = parse_interval_to_minutes(interval_name)

    # Quy t·∫Øc an to√†n: L√πi l·∫°i √≠t nh·∫•t 2 l·∫ßn ƒë·ªô d√†i n·∫øn + 10 ph√∫t tr·ªÖ
    # V√≠ d·ª•: 12h -> L√πi 24h. 1m -> L√πi 12 ph√∫t.
    buffer_minutes = (interval_minutes * 2) + 10

    last_time = get_last_processed_time(interval_name)
    cutoff_time = datetime.now() - timedelta(minutes=1)
    input_path = f"s3a://{S3_BUCKET}/silver/trades/"

    # ƒê·ªçc d·ªØ li·ªáu (T·ªëi ∆∞u: Ch·ªâ ƒë·ªçc c√°c file parquet c·∫ßn thi·∫øt n·∫øu c√≥ partition date)
    # ·ªû ƒë√¢y ƒë·ªçc full folder r·ªìi filter (Spark s·∫Ω t·ª± t·ªëi ∆∞u ƒë·∫©y filter xu·ªëng)
    df = spark.read.parquet(input_path)

    # if last_time:
    #     start_filter = last_time - timedelta(minutes=buffer_minutes)
    #     print(f"   ‚ÑπÔ∏è Incremental Mode: Reading trades after {start_filter} (Buffer: {buffer_minutes}m)")
    #     df = df.filter(F.col("trade_time") >= F.lit(start_filter))
    # else:
    #     print("   ‚ÑπÔ∏è Full Load Mode: Reading all trades")
    if last_time:
        # Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu c·∫ßn l·∫•y d·ªØ li·ªáu (tr·ª´ hao buffer)
        start_timestamp = last_time - timedelta(minutes=buffer_minutes)

        # --- K·ª∏ THU·∫¨T PARTITION PRUNING (QUAN TR·ªåNG) ---
        # Chuy·ªÉn ƒë·ªïi timestamp th√†nh chu·ªói ng√†y (YYYY-MM-DD)
        start_date_str = start_timestamp.strftime("%Y-%m-%d")

        print(f"   ‚ÑπÔ∏è Incremental Mode: Reading from {start_timestamp}")
        print(f"   üìÇ Partition Pruning: Reading folders from date_part >= {start_date_str}")

        # B∆∞·ªõc 1: L·ªçc Folder (Nhanh) - Spark s·∫Ω b·ªè qua c√°c folder c≈©
        # date_part v·∫´n c√≥ trong file parquet d∆∞·ª£c ƒë∆∞a l√™n silver nh∆∞ng s·∫Ω ko insert v√†o table DB
        df = df.filter(F.col("date_part") >= F.lit(start_date_str))

        # B∆∞·ªõc 2: L·ªçc chi ti·∫øt Time (Ch√≠nh x√°c) - L·∫•y ƒë√∫ng ph√∫t/gi√¢y
        df = df.filter(F.col("trade_time") >= F.lit(start_timestamp))

    else:
        print("   ‚ÑπÔ∏è Full Load Mode: Reading all partitions")
    try:
        # L·∫•y d√≤ng c√≥ th·ªùi gian nh·ªè nh·∫•t trong batch hi·ªán t·∫°i
        min_row = df.select(F.min("trade_time")).collect()
        min_data_time = min_row[0][0]

        if min_data_time is None:
            print("   ‚ö†Ô∏è No data found in this range.")
            return False

        print(f"   üïí Batch Min Data Time: {min_data_time}")
    except Exception as e:
        print(f"   ‚ö†Ô∏è Error checking min time: {e}")
        return False
    # 2. T√≠nh to√°n Features (Gi·ªØ nguy√™n logic ph·ª©c t·∫°p c·ªßa b·∫°n)
    df = df.withColumn("turnover", F.col("price") * F.col("quantity"))

    features_df = df.groupBy(
        F.col("symbol"),
        F.window(F.col("trade_time"), interval_window).alias("window")
    ).agg(
        # Basic
        F.min(F.struct("trade_time", "price")).getItem("price").alias("open"),
        F.max("price").alias("high"),
        F.min("price").alias("low"),
        F.max(F.struct("trade_time", "price")).getItem("price").alias("close"),
        F.sum("quantity").alias("volume"),
        F.sum("turnover").alias("total_turnover"),
        F.count("*").alias("trade_count"),
        # Advanced (Skew, Kurtosis...)
        F.stddev("price").alias("price_std"),
        F.skewness("price").alias("price_skew"),
        F.kurtosis("price").alias("price_kurtosis"),
        # percentile_approx l√† h√†m x·∫•p x·ªâ r·∫•t nhanh tr√™n Big Data
        F.percentile_approx("price", 0.25).alias("price_q25"),
        F.percentile_approx("price", 0.50).alias("price_median"),
        F.percentile_approx("price", 0.75).alias("price_q75"),
        # Buy/Sell Volume
        F.sum(F.when(F.col("side") == "buy", F.col("quantity")).otherwise(0)).alias("vol_buy"),
        F.sum(F.when(F.col("side") == "sell", F.col("quantity")).otherwise(0)).alias("vol_sell"),
        F.count(F.when(F.col("side") == "buy", 1)).alias("count_buy"),
        F.count(F.when(F.col("side") == "sell", 1)).alias("count_sell"),
        # --- D. SIZE DISTRIBUTION (Ph√¢n ph·ªëi kh·ªëi l∆∞·ª£ng l·ªánh) ---
        F.max("quantity").alias("size_max"),
        F.avg("quantity").alias("size_mean"),
        F.stddev("quantity").alias("size_std")
    )
    features_df = features_df.filter(
        (F.col("window.end") <= F.lit(cutoff_time)) &
        (F.col("window.start") >= F.lit(min_data_time))
    )

    # Ki·ªÉm tra xem c√≥ d·ªØ li·ªáu kh√¥ng sau khi l·ªçc
    if features_df.rdd.isEmpty():
        print(f"   ‚ö†Ô∏è No closed candles found for {interval_name}. Waiting for more data...")
        return False
    final_df = features_df.select(
        F.col("symbol"),
        F.col("window.start").alias("candle_time"),
        F.lit(interval_name).alias("interval"),
        "open", "high", "low", "close", "volume", "trade_count",
        "price_std","price_skew", "price_kurtosis", "price_q25","price_median","price_q75",
        "vol_buy", "vol_sell","count_buy","count_sell","size_max","size_mean","size_std",
        (F.col("total_turnover") / F.col("volume")).alias("vwap"),
        F.current_timestamp().alias("ingestion_time")
    )

    # 3. Ghi ra Staging Path (Ch·∫ø ƒë·ªô OVERWRITE cho folder staging n√†y th√¥i)
    # Folder n√†y ch·ªâ ch·ª©a data c·ªßa l·∫ßn ch·∫°y n√†y, kh√¥ng ch·ª©a data c≈©
    # Partition by date_part ƒë·ªÉ t·ªëi ∆∞u file size
    final_df = final_df.withColumn("date_part", F.date_format("candle_time", "yyyy-MM-dd"))

    # Path ri√™ng cho interval n√†y trong staging
    staging_path = f"s3a://{S3_BUCKET}/silver/agg_trades/{interval_name}"

    print(f"   üíæ Writing to Staging: {staging_path}")
    final_df.write.mode("overwrite").parquet(staging_path)

    return True
def merge_to_duckdb(interval_name):
    print(f"ü¶Ü Merging {interval_name} into DuckDB...")
    con = duckdb.connect(DUCKDB_PATH)
    with open('/opt/airflow/SQL_db/config_dw/warehouse_source.sql', 'r') as f:
        sql_script = f.read()
    con.execute(sql_script)
    staging_source = f"s3://{S3_BUCKET}/silver/agg_trades/{interval_name}/*.parquet"

    try:
        # Create Table (N·∫øu ch∆∞a c√≥)
        con.execute(f"""
            CREATE TABLE IF NOT EXISTS fact_market_features AS
            SELECT * FROM read_parquet('{staging_source}') LIMIT 0
        """)

        # X√≥a d·ªØ li·ªáu c≈© tr√πng l·∫∑p (D·ª±a tr√™n symbol, time V√Ä interval)
        # V√¨ ta ƒëang ch·∫°y loop, ch·ªâ x√≥a nh·ªØng d√≤ng thu·ªôc interval ƒëang x·ª≠ l√Ω
        print("   üîÑ Cleaning overlapping data...")
        con.execute(f"""
            DELETE FROM fact_market_features
            WHERE interval = '{interval_name}'
            AND (symbol, candle_time) IN (
                SELECT symbol, candle_time FROM read_parquet('{staging_source}')
            )
        """)

        # Insert m·ªõi
        print("   üì• Inserting new data...")
        con.execute(f"""
            INSERT INTO fact_market_features
            SELECT * FROM read_parquet('{staging_source}')
        """)

        print("   ‚úÖ Merge Complete.")

    except Exception as e:
        print(f"   ‚ö†Ô∏è DuckDB Merge Error: {e}")
    finally:
        con.close()
def agg_trades(spark):
    # Danh s√°ch Interval c·∫ßn ch·∫°y
    spark.sparkContext.setLocalProperty("spark.scheduler.pool", "parallel_pool")
    intervals = [
        ("1m", "1 minute"),
        ("5m", "5 minutes"),
        ("15m", "15 minutes"),
        ("1h", "1 hour"),
        ("4h", "4 hours"),
        ("12h", "12 hours") # Test interval l·ªõn
    ]

    print(f"STARTING BATCH PIPELINE FOR {len(intervals)} INTERVALS...")

    for name, window in intervals:
        # B∆∞·ªõc 1: T√≠nh to√°n
        calculate_features_incremental(spark, name, window)
        # B∆∞·ªõc 2: N·∫°p v√†o Fact
        merge_to_duckdb(name)
    spark.sparkContext.setLocalProperty("spark.scheduler.pool", None)
#agg_trades()