import duckdb
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, window, max, min, sum, struct, lit, date_format, current_timestamp

# --- CONFIG ---
S3_ENDPOINT = "http://localhost:9000"
S3_ACCESS = "minio"
S3_SECRET = "minio123"
S3_BUCKET = "trading-okx"

# ƒê∆∞·ªùng d·∫´n Checkpoint (B·∫ÆT BU·ªòC PH·∫¢I C√ì ƒë·ªÉ resume)
# Spark s·∫Ω l∆∞u tr·∫°ng th√°i v√†o ƒë√¢y. Tuy·ªát ƒë·ªëi kh√¥ng x√≥a th∆∞ m·ª•c n√†y n·∫øu mu·ªën ch·∫°y ti·∫øp.
CHECKPOINT_ROOT = f"s3a://{S3_BUCKET}/checkpoints/ohlc_parquet_v1/"
DUCKDB_PATH = '/mnt/d/learn/DE/Semina_project/datawarehouse.duckdb'
# C·∫•u h√¨nh ƒë·ªô tr·ªÖ cho ph√©p (Watermark)
# D·ªØ li·ªáu ƒë·∫øn mu·ªôn qu√° 10 ph√∫t s·∫Ω b·ªã b·ªè qua, n·∫øn s·∫Ω ƒë√≥ng sau 10 ph√∫t.
WATERMARK_DELAY = "10 minutes"
def get_spark():
    return SparkSession.builder \
        .appName("OKX_Bronze_To_Silver_trade") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4") \
        .config("spark.hadoop.fs.s3a.endpoint", S3_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", S3_ACCESS) \
        .config("spark.hadoop.fs.s3a.secret.key", S3_SECRET) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.sql.shuffle.partitions", "1") \
        .getOrCreate()
def get_duckdb_conn():
    con = duckdb.connect(DUCKDB_PATH)
    with open('/mnt/d/learn/DE/Semina_project/SQL_db/config_dw/warehouse_source.sql', 'r') as f:
             sql_script = f.read()
    con.execute(sql_script)
    return con
def load_to_duckdb(interval_name):
    """
    H√†m n√†y ch·∫°y SAU KHI Spark ƒë√£ ghi xong file Parquet.
    N√≥ ra l·ªánh cho DuckDB ƒë·ªçc file Parquet m·ªõi v√† insert v√†o b·∫£ng.
    """
    print(f"ü¶Ü [DuckDB] ƒêang n·∫°p d·ªØ li·ªáu OHLC ({interval_name}) v√†o Warehouse...")
    con = get_duckdb_conn()

    # Spark partition theo: interval=1h/date_part=2023-12-01
    parquet_source = f"s3://{S3_BUCKET}/silver/calculated_ohlc/interval={interval_name}/*/*.parquet"

    try:
        # 2.1 Load v√†o Staging (Incremental)
        # B·∫£ng staging: ohlc
        # L∆∞u √Ω: Interval l√† hardcode t·ª´ tham s·ªë h√†m v√¨ Spark partition theo folder n√†y
        con.execute(f"""
            INSERT INTO ohlc (symbol, candle_time, open, high, low, close, volume, interval)
            SELECT symbol, candle_time, open, high, low, close, volume, '{interval_name}'
            FROM read_parquet('{parquet_source}', hive_partitioning=1)
            WHERE candle_time > (SELECT COALESCE(MAX(candle_time), '1970-01-01'::TIMESTAMP) FROM ohlc WHERE interval = '{interval_name}')
        """)
        print("   ‚úÖ Staging Loaded.")

        # 2.2 Update Dim Time
        # T·∫°o ng√†y m·ªõi n·∫øu ch∆∞a c√≥
        con.execute("""
            INSERT INTO dim_time
            SELECT DISTINCT
                CAST(strftime(candle_time, '%Y%m%d') AS INTEGER) as date_key,
                CAST(candle_time AS DATE),
                EXTRACT(YEAR FROM candle_time), EXTRACT(QUARTER FROM candle_time),
                EXTRACT(MONTH FROM candle_time), EXTRACT(DAY FROM candle_time),
                ISODOW(candle_time), CASE WHEN ISODOW(candle_time) IN (6, 7) THEN TRUE ELSE FALSE END
            FROM ohlc
            WHERE CAST(strftime(candle_time, '%Y%m%d') AS INTEGER) NOT IN (SELECT date_key FROM dim_time)
        """)

        # 2.3 Insert Fact Table (fact_ohlc_calculated)
        # JOIN v·ªõi dim_symbol v√† dim_time ƒë·ªÉ l·∫•y Key chu·∫©n
        con.execute(f"""
            INSERT INTO fact_ohlc_calculated (symbol_key, date_key, interval, candle_time, open, high, low, close, volume)
            SELECT
                d.symbol_key,
                CAST(strftime(s.candle_time, '%Y%m%d') AS INTEGER) as date_key,
                s.interval,
                s.candle_time,
                s.open, s.high, s.low, s.close, s.volume
            FROM ohlc s
            JOIN dim_symbol d ON s.symbol = d.symbol_code
            WHERE s.candle_time > (SELECT COALESCE(MAX(candle_time), '1970-01-01'::TIMESTAMP) FROM fact_ohlc_calculated WHERE interval = '{interval_name}')
            AND s.interval = '{interval_name}'
        """)
        print("   ‚úÖ Gold (Fact) Loaded.")

    except Exception as e:
        print(f"   ‚ö†Ô∏è DuckDB Load Error: {e}")
        # Kh√¥ng raise ƒë·ªÉ pipeline ch·∫°y ti·∫øp interval kh√°c
    finally:
        con.close()

def run_streaming_ohlc(spark, interval_name="1m", interval_window="1 minute"):
    print(f"üöÄ ƒêang x·ª≠ l√Ω khung th·ªùi gian: {interval_name}")

    # 1. INPUT: READ STREAM (Ch·ªâ ƒë·ªçc file m·ªõi)
    # Spark t·ª± theo d√µi file n√†o m·ªõi trong th∆∞ m·ª•c n√†y
    input_path = f"s3a://{S3_BUCKET}/silver/trades/"

    # L·∫•y schema t·ª´ 1 file m·∫´u (ƒë·ªÉ tr√°nh l·ªói schema evolution)
    try:
        schema = spark.read.parquet(input_path).schema
    except:
        print("‚ö†Ô∏è Ch∆∞a c√≥ data trades. Tho√°t.")
        return

    df_trades = spark.readStream \
        .schema(schema) \
        .format("parquet") \
        .option("maxFilesPerTrigger", 1000) \
        .load(input_path)

    # 2. TRANSFORM: AGGREGATE V·ªöI WATERMARK
    # B·∫Øt bu·ªôc ph·∫£i c√≥ withWatermark ƒë·ªÉ d√πng mode 'append'

    df_ohlc = df_trades \
        .withWatermark("trade_time", WATERMARK_DELAY) \
        .groupBy(
            col("symbol"),
            window(col("trade_time"), interval_window).alias("window_time")
        ).agg(
            min(struct(col("trade_time"), col("price"))).getItem("price").alias("open"),
            max("price").alias("high"),
            min("price").alias("low"),
            max(struct(col("trade_time"), col("price"))).getItem("price").alias("close"),
            sum("quantity").alias("volume")
        )

    # Chu·∫©n h√≥a ƒë·∫ßu ra
    df_final = df_ohlc.select(
        col("symbol"),
        col("window_time.start").alias("candle_time"),
        col("open"), col("high"), col("low"), col("close"), col("volume"),
        lit(interval_name).alias("interval"),
        date_format(col("window_time.start"), "yyyy-MM-dd").alias("date_part")
    )

    # 3. OUTPUT: WRITE STREAM (APPEND ONLY)
    # Output path ri√™ng cho t·ª´ng interval
    output_path = f"s3a://{S3_BUCKET}/silver/calculated_ohlc/"
    checkpoint_path = f"{CHECKPOINT_ROOT}/{interval_name}"
    # Checkpoint ri√™ng cho t·ª´ng interval (Quan tr·ªçng!)

    query = df_final.writeStream \
        .format("parquet") \
        .outputMode("append") \
        .option("checkpointLocation", checkpoint_path) \
        .trigger(availableNow=True) \
        .partitionBy("interval", "date_part") \
        .start(output_path)

    # trigger(availableNow=True):
    # - ƒê·ªçc h·∫øt data m·ªõi -> T√≠nh to√°n -> Ghi xu·ªëng Parquet -> L∆∞u Checkpoint -> Stop.
    # - Kh√¥ng treo m√°y ch·ªù data nh∆∞ streaming th√¥ng th∆∞·ªùng.

    query.awaitTermination()

    print(f"‚úÖ Ho√†n t·∫•t x·ª≠ l√Ω {interval_name}. Data ƒë√£ ƒë∆∞·ª£c Append v√†o MinIO.")
    load_to_duckdb(interval_name)

def main():
    spark = get_spark()
    spark.sparkContext.setLogLevel("ERROR")

    # B·∫°n c√≥ th·ªÉ ch·∫°y nhi·ªÅu interval
    intervals = [
        ("1m", "1 minute"),
        ("5m", "5 minute")
    ]

    for name, window_duration in intervals:
        run_streaming_ohlc(spark, name, window_duration)

    spark.stop()
main()