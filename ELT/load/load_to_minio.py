# import json
# import gzip
# import threading
# from datetime import datetime
# from confluent_kafka import Consumer
# import boto3
# import logging
# # --- CONFIG LOGGING ---
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s | %(levelname)s | %(message)s',
#     datefmt='%Y-%m-%d %H:%M:%S'
# )
# logger = logging.getLogger("airflow.task")
#
# # --- CONFIG APP ---
# KAFKA_BOOTSTRAP = "redpanda:29092"
# TOPICS = ["okx_trades", "okx_funding", "okx_orderbook", "okx_ohlc_mark", "okx_ohlc_index"]
# S3_ENDPOINT = "http://minio:9000"
# S3_ACCESS = "minio"
# S3_SECRET = "minio123"
# S3_BUCKET = "trading-okx"
# FLUSH_INTERVAL = 10
# MAX_BUFFER_BYTES = 1 * 1024  # 200KB (C·∫•u h√¨nh th·ª±c t·∫ø h·ª£p l√Ω h∆°n 1KB)
#
# # --- SETUP S3 & KAFKA ---
# s3 = boto3.client(
#     "s3",
#     endpoint_url=S3_ENDPOINT,
#     aws_access_key_id=S3_ACCESS,
#     aws_secret_access_key=S3_SECRET,
#     region_name="us-east-1"
# )
#
#
#
# buffers = {}
# buf_lock = threading.Lock()
# stop_event = threading.Event()
#
#
# # --- FUNCTIONS ---
#
# def s3_put_bytes(file_key, data_bytes):
#     """Upload data l√™n MinIO"""
#     try:
#         try:
#             s3.head_bucket(Bucket=S3_BUCKET)
#         except Exception:
#             logger.warning(f"Bucket {S3_BUCKET} not found, creating...")
#             s3.create_bucket(Bucket=S3_BUCKET)
#
#         # Upload
#         s3.put_object(Bucket=S3_BUCKET, Key=file_key, Body=data_bytes)
#         logger.info(f"Uploaded: {file_key} ({len(data_bytes)} bytes)")
#         return True
#     except Exception as e:
#         logger.error(f"Upload ERROR: {e}")
#         return False
# def flush_key(k):
#     """L·∫•y data t·ª´ buffer, n√©n Gzip v√† upload (D√πng cho Scheduled Flush)"""
#     with buf_lock:
#         if k not in buffers or len(buffers[k]) == 0:
#             return
#
#         # L·∫•y data v√† x√≥a buffer ngay l·∫≠p t·ª©c
#         data = bytes(buffers[k])
#         buffers[k] = bytearray()
#
#     try:
#         topic, dh = k.split("|")
#         date, hour = dh.rsplit("-", 1)
#         ts = datetime.now().strftime("%Y%m%d_%H%M%S")
#
#         object_key = f"bronze/{topic}/{date}/{hour}/{topic}_{ts}.jsonl.gz"
#
#         # --- FIX QUAN TR·ªåNG: N√âN GZIP TR∆Ø·ªöC KHI UPLOAD ---
#         compressed_data = gzip.compress(data)
#
#         s3_put_bytes(object_key, compressed_data)
#
#     except Exception as e:
#         logger.error(f"Error flushing key {k}: {e}")
# def flush_thread_func():
#     """Thread ch·∫°y ng·∫ßm ƒë·ªãnh k·ª≥"""
#     logger.info("‚è≥ Flush thread started")
#     while not stop_event.is_set():
#         if stop_event.wait(FLUSH_INTERVAL):
#             break
#
#         with buf_lock:
#             keys = list(buffers.keys())
#
#         for k in keys:
#             if len(buffers[k]) > 0:
#                 flush_key(k)
#     logger.info("‚è≥ Flush thread stopped.")
# def flush_all_remaining():
#     logger.info("üíæ Flushing ALL remaining data...")
#     with buf_lock:
#         keys = list(buffers.keys())
#
#     count = 0
#     for k in keys:
#         if len(buffers[k]) > 0:
#             flush_key(k)
#             count += 1
#     logger.info(f"üèÅ Final flush completed. Processed {count} keys.")
# def add_record(topic, record):
#     ts = record.get("received_at")
#     try:
#         dt = datetime.fromisoformat(ts) if ts else datetime.now()
#     except:
#         dt = datetime.now()
#
#     date = dt.strftime("%Y-%m-%d")
#     hour = dt.strftime("%H")
#     key = f"{topic}|{date}-{hour}"
#
#     # Chu·∫©n b·ªã d√≤ng JSONL
#     line = json.dumps(record) + "\n"
#
#     # L∆∞u √Ω: Ta l∆∞u text (bytes) v√†o buffer, ch·ªâ n√©n khi upload
#     line_bytes = line.encode('utf-8')
#
#     with buf_lock:
#         if key not in buffers:
#             buffers[key] = bytearray()
#         buffers[key].extend(line_bytes)
#
#         current_len = len(buffers[key])
#
#         # FLUSH NGAY L·∫¨P T·ª®C N·∫æU ƒê·∫¶Y
#         if current_len >= MAX_BUFFER_BYTES:
#             logger.info(f"‚ö° Buffer full for {key} ({current_len} bytes), flushing...")
#
#             # Copy data ra v√† clear buffer
#             data_to_upload = bytes(buffers[key])
#             buffers[key] = bytearray()
#
#             # T·∫°o ƒë∆∞·ªùng d·∫´n (Logic gi·ªëng h·ªát flush_key)
#             filepath = f"bronze/{topic}/{date}/{hour}/{topic}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl.gz"
#
#             # Ch·∫°y thread ri√™ng ƒë·ªÉ upload (ƒë√£ fix n√©n Gzip)
#             threading.Thread(
#                 target=lambda: s3_put_bytes(
#                     filepath,
#                     gzip.compress(data_to_upload)  # <--- ƒê√É C√ì GZIP ·ªû ƒê√ÇY (Code c≈© c·ªßa b·∫°n ƒë√∫ng ch·ªó n√†y)
#                 ),
#                 daemon=True
#             ).start()
# def load_data_to_minio():
#     c = Consumer({
#         "bootstrap.servers": KAFKA_BOOTSTRAP,
#         "group.id": "trade-bronze",  # ƒê·ªïi group ID m·ªõi ƒë·ªÉ ƒë·ªçc l·∫°i t·ª´ ƒë·∫ßu cho ch·∫Øc
#         "auto.offset.reset": "earliest"
#     })
#     c.subscribe(TOPICS)
#     t = threading.Thread(target=flush_thread_func)
#     t.start()
#
#     print(f"üöÄ Consumer started. Listening on {KAFKA_BOOTSTRAP}")
#     try:
#         while True:
#             msg = c.poll(1.0)
#             if msg is None:
#                 continue
#             if msg.error():
#                 logger.error(f"Kafka error: {msg.error()}")
#                 continue
#
#             try:
#                 val = json.loads(msg.value().decode())
#                 add_record(msg.topic(), val)
#                 # print(".", end="", flush=True) # Uncomment n·∫øu mu·ªën xem dot log
#             except Exception as e:
#                 logger.error(f"Decode error: {e}")
#
#     except KeyboardInterrupt:
#         logger.info("\nüõë User stopped consumer...")
#     finally:
#         stop_event.set()
#         t.join()
#         flush_all_remaining()
#         c.close()
#         logger.info("üëã Consumer stopped gracefully.")
# #load_data_to_miniov()
import json
import gzip
import threading
import time
import logging
from datetime import datetime
from confluent_kafka import Consumer
import boto3

# --- CONFIG LOGGING ---
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger("airflow.task")

# --- GLOBAL VARIABLES ---
# Ch·ªâ gi·ªØ l·∫°i c√°c bi·∫øn c·∫•u tr√∫c d·ªØ li·ªáu, KH√îNG kh·ªüi t·∫°o connection ·ªü ƒë√¢y
buffers = {}
buf_lock = threading.Lock()
stop_event = threading.Event()
s3_client = None  # S·∫Ω kh·ªüi t·∫°o b√™n trong h√†m

# --- CONSTANTS DEFAULT ---
# C√°c gi√° tr·ªã m·∫∑c ƒë·ªãnh n·∫øu DAG kh√¥ng truy·ªÅn v√†o
DEFAULT_TOPICS = ["okx_trades", "okx_funding", "okx_orderbook", "okx_ohlc_mark", "okx_ohlc_index"]
DEFAULT_BUCKET = "trading-okx"


# --- FUNCTIONS ---

def get_s3_client(endpoint, access_key, secret_key):
    """T·∫°o k·∫øt n·ªëi S3 an to√†n b√™n trong h√†m"""
    return boto3.client(
        "s3",
        endpoint_url=endpoint,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        region_name="us-east-1"
    )


def s3_put_bytes(file_key, data_bytes, bucket_name):
    """Upload data l√™n MinIO d√πng client to√†n c·ª•c ƒë√£ init"""
    global s3_client
    if not s3_client:
        logger.error("‚ùå S3 Client ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o!")
        return False

    try:
        # Ki·ªÉm tra bucket t·ªìn t·∫°i (cache 1 l·∫ßn ƒë·ªÉ t·ªëi ∆∞u)
        if not hasattr(s3_put_bytes, "bucket_checked"):
            try:
                s3_client.head_bucket(Bucket=bucket_name)
            except Exception:
                logger.warning(f"‚ö†Ô∏è Bucket {bucket_name} ch∆∞a c√≥, ƒëang t·∫°o...")
                s3_client.create_bucket(Bucket=bucket_name)
            s3_put_bytes.bucket_checked = True

        # Upload
        s3_client.put_object(Bucket=bucket_name, Key=file_key, Body=data_bytes)
        logger.info(f"‚úÖ Uploaded: {file_key} ({len(data_bytes)} bytes)")
        return True
    except Exception as e:
        logger.error(f"‚ùå Upload ERROR: {e}")
        return False


def flush_key(k, bucket_name):
    """L·∫•y data t·ª´ buffer, n√©n Gzip v√† upload"""
    with buf_lock:
        if k not in buffers or len(buffers[k]) == 0:
            return
        data = bytes(buffers[k])
        buffers[k] = bytearray()  # X√≥a buffer ngay

    try:
        topic, dh = k.split("|")
        date, hour = dh.rsplit("-", 1)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")

        # ƒê∆∞·ªùng d·∫´n: bronze/topic/date/hour/filename
        object_key = f"bronze/{topic}/{date}/{hour}/{topic}_{ts}.jsonl.gz"

        # N√©n Gzip
        compressed_data = gzip.compress(data)

        # G·ªçi h√†m upload
        s3_put_bytes(object_key, compressed_data, bucket_name)

    except Exception as e:
        logger.error(f"Error flushing key {k}: {e}")


def flush_thread_func(interval, bucket_name):
    """Thread ch·∫°y ng·∫ßm ƒë·ªãnh k·ª≥ flush data"""
    logger.info("‚è≥ Flush thread started")
    while not stop_event.is_set():
        if stop_event.wait(interval):
            break

        # Snapshot keys
        with buf_lock:
            keys = list(buffers.keys())

        for k in keys:
            with buf_lock:
                has_data = len(buffers[k]) > 0

            if has_data:
                flush_key(k, bucket_name)

    logger.info("‚è≥ Flush thread stopped.")


def flush_all_remaining(bucket_name):
    logger.info("üíæ Flushing ALL remaining data...")
    with buf_lock:
        keys = list(buffers.keys())
    for k in keys:
        flush_key(k, bucket_name)


def add_record(topic, record, max_bytes, bucket_name):
    ts = record.get("received_at")
    try:
        dt = datetime.fromisoformat(ts) if ts else datetime.now()
    except:
        dt = datetime.now()

    date = dt.strftime("%Y-%m-%d")
    hour = dt.strftime("%H")
    key = f"{topic}|{date}-{hour}"

    line = json.dumps(record) + "\n"
    line_bytes = line.encode('utf-8')

    with buf_lock:
        if key not in buffers:
            buffers[key] = bytearray()
        buffers[key].extend(line_bytes)
        current_len = len(buffers[key])

    # Flush ngay n·∫øu ƒë·∫ßy buffer
    if current_len >= max_bytes:
        logger.info(f"‚ö° Buffer full for {key} ({current_len} bytes), flushing...")
        flush_key(key, bucket_name)


# --- MAIN ENTRY POINT CHO AIRFLOW ---
def load_data_to_minio(
        kafka_bootstrap="redpanda:29092",
        s3_endpoint="http://minio:9000",
        s3_access="minio",
        s3_secret="minio123",
        s3_bucket=DEFAULT_BUCKET,
        runtime_seconds=3500  # Default 58 ph√∫t
):
    """
    H√†m ch√≠nh:
    1. Nh·∫≠n tham s·ªë t·ª´ DAG (quan tr·ªçng!).
    2. Ch·∫°y c√≥ th·ªùi h·∫°n (runtime_seconds) r·ªìi t·ª± d·ª´ng.
    """
    global s3_client

    # 1. Kh·ªüi t·∫°o S3 Client (Quan tr·ªçng: Init b√™n trong h√†m)
    s3_client = get_s3_client(s3_endpoint, s3_access, s3_secret)

    logger.info(f"üöÄ Starting Loader Task.")
    logger.info(f"   Kafka: {kafka_bootstrap}")
    logger.info(f"   MinIO: {s3_endpoint}")
    logger.info(f"   Bucket: {s3_bucket}")
    logger.info(f"   Duration: {runtime_seconds}s")

    # 2. C·∫•u h√¨nh Consumer
    c = Consumer({
        "bootstrap.servers": kafka_bootstrap,
        "group.id": "airflow-minio-loader",
        "auto.offset.reset": "earliest",
        "enable.auto.commit": True
    })

    try:
        c.subscribe(DEFAULT_TOPICS)
        logger.info(f"üéß Subscribed to: {DEFAULT_TOPICS}")
    except Exception as e:
        logger.error(f"‚ùå Failed to subscribe: {e}")
        return

    # 3. Start Flush Thread
    stop_event.clear()
    # Truy·ªÅn tham s·ªë bucket v√†o thread
    t = threading.Thread(target=flush_thread_func, args=(10, s3_bucket))
    t.start()

    start_time = time.time()
    msg_count = 0

    try:
        # --- LOGIC TH·ªúI GIAN: Ch·∫°y X gi√¢y r·ªìi d·ª´ng ---
        while (time.time() - start_time) < runtime_seconds:
            msg = c.poll(1.0)

            if msg is None:
                continue
            if msg.error():
                logger.error(f"Kafka error: {msg.error()}")
                continue

            try:
                val = json.loads(msg.value().decode())
                add_record(msg.topic(), val, 1024 * 1024, s3_bucket)  # 1MB buffer limit
                msg_count += 1

                if msg_count % 5000 == 0:
                    logger.info(f"Processed {msg_count} messages...")
            except Exception as e:
                logger.error(f"Decode error: {e}")

        logger.info(f"‚è∞ Time limit reached ({runtime_seconds}s). Stopping task...")

    except KeyboardInterrupt:
        logger.info("üõë Stopped manually.")
    except Exception as e:
        logger.error(f"Critical Error: {e}")
        raise e
    finally:
        # D·ªçn d·∫πp
        stop_event.set()
        t.join()
        flush_all_remaining(s3_bucket)
        c.close()
        logger.info("üëã Loader stopped gracefully.")


# N·∫øu mu·ªën test th·ªß c√¥ng b·∫±ng l·ªánh python load_to_minio.py
if __name__ == "__main__":
    load_data_to_minio(runtime_seconds=60)  # Ch·∫°y th·ª≠ 60s r·ªìi d·ª´ng