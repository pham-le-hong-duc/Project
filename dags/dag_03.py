# from airflow import DAG
# from airflow.operators.python import PythonOperator
# from datetime import timedelta
# from airflow.utils.dates import days_ago
# import threading
# from plugins.spark_config import get_spark
# from ELT.transform.trans_tables_db import trans_table
# from ELT.transform.agg_orderbook import agg_orderbook
# from ELT.transform.agg_trades import agg_trades
# from ELT.transform.index_price_kline import index_price
# from ELT.transform.mark_price_kline import mark_price
# from ELT.transform.merge_features import process_and_merge_all
# from ELT.transform.derives_features import derives_features
#
#
# def run_pipeline():
#     print("ğŸš€ [MASTER] STARTING PIPELINE (ONE SPARK SESSION)...")
#
#     # A. Khá»Ÿi táº¡o Spark Session (DUY NHáº¤T Táº I ÄÃ‚Y)
#     spark = get_spark()
#     spark.sparkContext.setLogLevel("ERROR")
#     try:
#         # BÆ¯á»šC 1: trans_table (Cháº¡y tuáº§n tá»±)
#         print("\nğŸ”¹ Step 1: Running trans_table...")
#         trans_table(spark)
#
#         # BÆ¯á»šC 2: Cháº¡y Song Song (Parallel Group)
#         # agg_orderbook, agg_trades, index_price, mark_price
#         print("\nğŸ”¹ Step 2: Running Aggregations in Parallel...")
#         # Äá»‹nh nghÄ©a cÃ¡c luá»“ng (Thread)
#         t_orderbook = threading.Thread(target=agg_orderbook, args=(spark,))
#         t_trades = threading.Thread(target=agg_trades, args=(spark,))
#         t_index = threading.Thread(target=index_price, args=(spark,))
#         t_mark = threading.Thread(target=mark_price, args=(spark,))
#
#         # Báº¯t Ä‘áº§u cháº¡y táº¥t cáº£ cÃ¹ng lÃºc
#         t_orderbook.start()
#         t_trades.start()
#         t_index.start()
#         t_mark.start()
#
#         # Chá» táº¥t cáº£ cÃ¡c luá»“ng xong má»›i Ä‘i tiáº¿p (Join)
#         t_orderbook.join()
#         t_trades.join()
#         t_index.join()
#         t_mark.join()
#         print("   âœ… All parallel aggregations finished.")
#
#         # BÆ¯á»šC 3: process_and_merge_all (Cháº¡y sau khi nhÃ³m trÃªn xong)
#         print("\nğŸ”¹ Step 3: Running process_and_merge_all...")
#         process_and_merge_all(spark)
#
#         # BÆ¯á»šC 4: derives_features (Cháº¡y cuá»‘i cÃ¹ng)
#         print("\nğŸ”¹ Step 4: Running derives_features...")
#         derives_features(spark)
#
#         print("\nğŸ† [MASTER] PIPELINE COMPLETED SUCCESSFULLY!")
#
#     except Exception as e:
#         print(f"âŒ [MASTER] CRITICAL ERROR: {e}")
#         raise e  # BÃ¡o lá»—i Ä‘á»ƒ Airflow retry
#     finally:
#         # C. Táº¯t Spark Session (DÃ¹ lá»—i hay khÃ´ng cÅ©ng pháº£i táº¯t)
#         print("ğŸ›‘ Stopping Spark Session...")
#         spark.stop()
#
#
# default_args = {
#     'owner': 'trading-data',
#     'retries': 1,
#     'retry_delay': timedelta(minutes=1)
# }
#
# with DAG(
#         'transform_data_silver_and_gold',
#         default_args=default_args,
#         schedule_interval='*/5 * * * *',  # Cháº¡y má»—i 5 phÃºt
#         start_date=days_ago(1),
#         catchup=False,
#         max_active_runs=1
# ) as dag:
#     task_master_run = PythonOperator(
#         task_id='run_pipeline',
#         python_callable=run_pipeline
#     )

''' Khong dung threading'''
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import timedelta
from airflow.utils.dates import days_ago
from plugins.spark_config import get_spark
from ELT.transform.trans_tables_db import trans_table
from ELT.transform.agg_orderbook import agg_orderbook
from ELT.transform.agg_trades import agg_trades
from ELT.transform.index_price_kline import index_price
from ELT.transform.mark_price_kline import mark_price
from ELT.transform.merge_features import process_and_merge_all
from ELT.transform.derives_features import derives_features


def run_pipeline():
    print("ğŸš€ [MASTER] STARTING PIPELINE (SEQUENTIAL MODE)...")

    # A. Khá»Ÿi táº¡o Spark Session
    # LÆ°u Ã½: Spark Session nÃ y sáº½ sá»‘ng suá»‘t quÃ¡ trÃ¬nh cháº¡y Task
    spark = get_spark()
    spark.sparkContext.setLogLevel("ERROR")

    try:
        # --- BÆ¯á»šC 1: ETL CÆ  Báº¢N (Raw -> Silver -> Gold) ---
        print("\nğŸ”¹ Step 1: Running trans_table (ETL Basic)...")
        trans_table(spark)

        # --- BÆ¯á»šC 2: AGGREGATIONS (Cháº¡y tuáº§n tá»± Ä‘á»ƒ tiáº¿t kiá»‡m RAM) ---
        print("\nğŸ”¹ Step 2: Running Aggregations...")

        print("   -> Running agg_orderbook...")
        agg_orderbook(spark)

        print("   -> Running agg_trades...")
        agg_trades(spark)

        print("   -> Running index_price...")
        index_price(spark)

        print("   -> Running mark_price...")
        mark_price(spark)

        # --- BÆ¯á»šC 3: MERGE FEATURES ---
        print("\nğŸ”¹ Step 3: Running process_and_merge_all...")
        process_and_merge_all(spark)

        # --- BÆ¯á»šC 4: DERIVED FEATURES ---
        print("\nğŸ”¹ Step 4: Running derives_features...")
        derives_features(spark)

        print("\nğŸ† [MASTER] PIPELINE COMPLETED SUCCESSFULLY!")

    except Exception as e:
        print(f"âŒ [MASTER] CRITICAL ERROR: {e}")
        # Quan trá»ng: Raise lá»—i Ä‘á»ƒ Airflow biáº¿t lÃ  Task Failed vÃ  cÃ³ thá»ƒ Retry
        raise e
    finally:
        # C. LuÃ´n táº¯t Spark Session Ä‘á»ƒ giáº£i phÃ³ng RAM cho Docker
        print("ğŸ›‘ Stopping Spark Session...")
        spark.stop()


default_args = {
    'owner': 'trading-data',
    'retries': 1,  # Retry 1 láº§n náº¿u lá»—i
    'retry_delay': timedelta(minutes=1)
}

with DAG(
        '03_transform_master_dag',  # Äáº·t tÃªn DAG thá»‘ng nháº¥t (báº¯t Ä‘áº§u báº±ng sá»‘ thá»© tá»±)
        default_args=default_args,
        schedule_interval='*/5 * * * *',  # Cháº¡y má»—i 5 phÃºt
        start_date=days_ago(1),
        catchup=False,
        max_active_runs=1  # Chá»‰ cho phÃ©p 1 DAG cháº¡y táº¡i 1 thá»i Ä‘iá»ƒm (trÃ¡nh chá»“ng chÃ©o)
) as dag:
    task_master_run = PythonOperator(
        task_id='run_pipeline',
        python_callable=run_pipeline
    )